---
title: "Deep Learning con conjunto de datos Titanic"
output:
  html_document:
    df_print: paged
html_notebook: default
---

# Introducción al problema 

Este cuaderno trata de resolver el tercer ejercicio propuesto que consiste en mejorar los resultados obtenidos con respecto a una primera versión, en la que se entrena un modelo con redes neuronales para predecir si los pasajeros del Titanic sobrevivieron o no. Para ello vamos a hacer uso del mismo fichero que contiene el conjunto de entrenamiento situado en el repositorio de GitHub de la asignatura.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(32)
```

# Estudio de los datos

Comenzamos leyendo el fichero de entrenamiento al completo y a continuación estudiamos el estado de los datos para comprobar aspectos tales como el porcentaje de valores perdidos en cada columna, además de cómo de balanceada está la clase a predecir. Como podemos observar existen tres columnas con valores perdidos con, aproximadamente un 0.2%, 20% y 77% respectivamente. En los dos primeros casos esta cifra no es demasiado elevada y por tanto se pueden aplicar técnicas para tratar de imputarlos, mientras que para la tercera columna el número de NAs es bastante superior al de valores normales por lo que en principio se descarta. 
Asimismo, también podemos observar algunas columnas que no parecen ser relevantes para determinar si un pasajero sobrevivió o no, como el identificador del registro, su nombre o el número de ticket. Estos atributos también pueden ser omitidos.

En relación al balanceado de la clase a predecir podemos observar que existe un 61.62% de casos negativos y el resto positivos. Si bien podemos determinar que la clase se encuentra desbalanceada, no es tan exagerado como el *dataset* de la práctica 1.

```{r message=FALSE, warning=FALSE}
library(readr)
library(funModeling)
# Leemos el fichero de entrenamiento
train<-read_csv('train.csv')
# Estudiamos el estado de los datos
df_status(train)
```

# Preprocesamiento de datos

En base al análisis anteriormente realizado y al *script* de ejemplo para este ejercicio se proponen las siguientes técnicas de preprocesamiento.
* Se seleccionan los mismos atributos predictivos que en la primera versión de este ejercicio: `Pclass`, `Sex`, `Age`, `Fare`, puesto que conocemos su buena capacidad para predecir la variable `Survived`.
* Imputamos los valores perdidos utilizando la función `mice` [1] puesto que ofrece la posibilidad de aplicar distintos métodos para predecir dichos valores. Esto es muy útil puesto que a veces las técnicas más sofitiscadas como clasificación o árboles de regresión pueden o disponer de información suficiente como para predecir el valor y, por tanto, hay que recurrir a técnicas más sencillas como la media.
* Balanceamos la clase `Survived` aplicando la técnica conocida como *oversampling* de modo que generemos un mayor número de datos de la clase minoritaria. Para ello voy a utilizar la función `SMOTE` [2] puesto que es capaz de buscar un equilibrio entre el número de muestras de ambas clases. Además podremos disponer de un conjunto de datos más amplio, lo que puede suponer un plus a la hora de entrenar los modelos posteriormente.
* Dividimos el conjunto completo en entrenamiento y validación para poder entrenar las distintas redes neuronales y testearlas posteriormente para comprobar la calidad de los modelos.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}

conjunto_inicial<-function() {
  library(magrittr)
  library(dplyr)
  # Seleccionamos las columnas predictoras y cambiamos el tipo
  # de la columna 'Sex' a numérico
  datos_finales<-train %>%
    select(Survived, Pclass, Sex, Age, Fare) %>%
    mutate(Sex = as.numeric(as.factor(Sex)) - 1)
  datos_finales
}

imputar_nas<-function(datos, imputs, iters, metodo) {
  library(mice)
  # Especificamos el número de imputaciones, el número de iteraciones
  # por imputación y el método a utilizar para imputar los NAs
  modelo_mice<-mice(datos, m=imputs, maxit=iters, method=metodo)
  # Obtenemos el conjunto de datos imputado
  datos_imputados<-complete(modelo_mice)
  # Devolvemos el conjunto resultante
  datos_imputados
}

over_sampling<-function(datos, p_over, p_under, k_vecinos) {
  library(DMwR)
  # Oversampling con el algoritmo SMOTE para generar más muestras
  datos$Survived<-as.factor(datos$Survived)
  datos_finales<-SMOTE(Survived ~., datos, perc.over=p_over, perc.under=p_under, k=k_vecinos)
  # Devolvemos los datos resultantes
  datos_finales
}

get_train_test<-function(datos, porcentaje_train) {
  # Mezclamos los datos aleatoriamente
  set.seed(32)
  datos<-datos[sample(1:nrow(datos)), ]
  # Dividimos el conjunto en train y test
  library(caret)
  train<-createDataPartition(datos$Survived, p=porcentaje_train, list=FALSE, times=1)
  datos_train<-datos[train, ]
  datos_test<-datos[-train, ]
  # Devolvemos ambos dataframes en una lista
  df<-list()
  df$train<-datos_train
  df$test<-datos_test
  df
}
```

# Modelos

En esta sección vamos a entrenar diversos modelos aplicando diferentes técnicas de preprocesamiento así como modificando las características de la red neuronal. En primer lugar, con el objetivo de comprobar la influencia del tratamiento de datos en la capacidad de generalización de los clasificadores, vamos a aplicar técnicas como la imputación de NAs y el balanceado de clases para entrenar el modelo de ejemplo de GitHub. A continuación, en función de los mejores resultados, estableceremos un mismo preprocesamiento para comenzar a modificar la arquitectura y parámetros de la red neuronal.

Para ello presentamos, a continuación, tres funciones con las cuales podremos obtener en primer lugar los conjuntos de entrenamiento y validación en formato matriz para entrenar la red neuronal. Este cometido lo lleva a cabo la segunda función a la que se le pasa el modelo de red generado así como el conjunto de entrenamiento. La última función tiene como fin validar el modelo préviamente entrenado proporcionando el conjunto de validación. Todos estos pasos son comunes al desarrollo de todos los modelos y por tanto se han modelado como funciones para no repetir código.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
get_conjunto_matriz<-function(datos) {
  library(magrittr)
  library(dplyr)
  # Matriz de datos sin las etiquetas
  x_datos <- datos %>%
    select(-Survived) %>%
    data.matrix()
  # Etiquetas
  y_datos <- datos %>%
    select(Survived) %>%
    data.matrix()
  # Devolvemos ambos
  resultado<-list()
  resultado$dataset<-x_datos
  resultado$etiquetas<-y_datos
  resultado
}

entrenar_modelo<-function(modelo, x_train, y_train, loss, op, eps, batch, val) {
  # Entrenamos el modelo
  set.seed(32)
  modelo %>% compile(
    loss = loss,
    metrics = c('accuracy'),
    optimizer = op
  )
  
  # Ajustamos el modelo a los datos de entrenamiento
  history <- modelo %>% 
    fit(
      x_train, y_train, 
      epochs = eps, 
      batch_size = batch,
      validation_split = val
    )
  plot(history)
}

validar_modelo<-function(modelo, x_test, y_test) {
  # Validación del modelo
  modelo %>% evaluate(x_test, y_test)
  preds<-modelo %>% predict_classes(x_test)
  conf_matriz<-confusionMatrix(as.factor(y_test), as.factor(preds), positive='1')
  conf_matriz
}
```

## Modelo 1

En este primer modelo procedo a **imputar los NAs** del conjunto de datos. Si bien es aconsejable que el número de imputaciones e iteraciones sea lo más alto posible para asegurar que el algoritmo converge, creo que la siguiente configuración es la que mejor relación guarda entre sendos atributos y el tiempo que invierte en la imputación. La calidad la podremos comprobar cuando entrenemos un modelo con estos datos. En cuanto a la técnica se ha escogido *Random Forest* por ser popular por su robustez y versatilidad con casi cualquier conjunto de datos.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
# Obtenemos el conjunto inicial con las mejores variables predictoras
train_orig<-conjunto_inicial()
# Imputamos los valores perdidos
train_imput<-imputar_nas(train_orig, 10, 10, 'rf')
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_imput, 0.7)
```

Como se puede observar, el conjunto de entrenamiento cuenta con 624 registros mientras que el de validación dispone de 267 muestras. A continuación transformamos sendos a matrices y separamos las etiquetas para, posteriormente, proporcionarle a la red neuronal los conjuntos resultantes y así ajustar el modelo. Como se ha mencionado anteriormente, el modelo es el mismo que el del ejemplo para comprobar la influencia de la imputación de los valores perdidos. Y tal y como podemos comprobar, el **modelo resultante es bastante similar al del *script* inicial con un 68.54% de precisión**. La razón de ser reside en que en este conjunto no existía un gran número de NAs comparado con la cantidad de valores de cada muestra, por lo que para el modelo entrenado con redes neuronales no ha supuesto un cambio significativo como para proporcionar resultados súmamente distintos. 

```{r message=FALSE, warning=FALSE}
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Reproducimos el mismo modelo que el del ejemplo
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo <- keras_model_sequential()
modelo <- modelo %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(ncol(train_imput) - 1)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo, m_train$dataset, m_train$etiquetas, 
    'binary_crossentropy', 'adam', 20, 100, 0.10)
validar_modelo(modelo, m_test$dataset, m_test$etiquetas)
```

## Modelo 2

En este segundo modelo vamos a incluir un nuevo preprocesamiento consistente en aplicar **oversampling** para balancear las clases de la variable a predecir. Asimismo, también se persigue aumentar el número de muestras para comprobar si con un mayor número de ejemplos, la capacidad de predicción del clasificador mejora. Con este fin vamos a partir del conjunto imputado anterior y aplicaremos directamente la función `SMOTE` para llevar a cabo esta técnica. Tras realizar varios experimentos para ajustar los parámetros de la función, a continuación se presenta la configuración con la que se logra disponer del mismo número de muestras positivas y negativas (2.052). En total el nuevo conjunto dispone de **4.104 registros**.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
# Oversampling para balancear las clases
train_smote<-over_sampling(train_imput, 500, 120, 5)
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_smote, 0.7)
```

Lógicamente, al disponer de un mayor número de muestras al partir el conjunto original en entrenamiento y test estos disponen de un mayor número de ejemplos con los que entrenar y validar el clasificador, cuya configuración de nuevo será la misma que la del *script* inicial.

Como podemos observar, el clasificador entrenado en este caso proporciona **resultados son mucho peores** que en el caso anterior, ya que como podemos apreciar sus predicciones se basan en que todos sobrevivien y por tanto acierta un 50% de las veces, por estar las clases balanceadas. Por lo tanto, para este problema en concreto y esta tipología de red no es beneficioso aplicar *oversampling* sobre el conjunto de datos.

```{r message=FALSE, warning=FALSE}
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Reproducimos el mismo modelo que el del ejemplo
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo2 <- keras_model_sequential()
modelo2 <- modelo2 %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(ncol(train_smote) - 1)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo2, m_train$dataset, m_train$etiquetas, 
    'binary_crossentropy', 'adam', 20, 100, 0.10)
modelo2 %>% evaluate(m_test$dataset, m_test$etiquetas)
preds<-modelo2 %>% predict_classes(m_test$dataset)
length(preds[preds==0])
length(preds[preds==1])
```

## Modelo 3

En esta sección comenzamos a modificar la arquitectura de la red neuronal partiendo de un conjunto de datos al que se le han imputado los valores perdidos, pero mantiene el mismo número de registros. Este se corresponde con el conjunto de datos obtenido en el primer modelo entrenado.

El primer aspecto a considerar reside en establecer el número de capas de la red neuronal. Generalmente, la arquitectura más común es disponer de una capa de entrada, una capa oculta que realiza la extracción de características y una última capa, la de salida, que proporciona las predicciones para cada muestra. No obstante, dependiendo del problema que deseemos resolver esta arquitectura puede no ser la más adecuada. En nuestro caso, el número de variables predictoras es bastante bajo por lo que quizás este tipo de arquitectura más simple pueda obtener un mejor clasificador. Por ende vamos a establecer una capa de entrada cuyo número de neuronas es el mismo que el número de entradas, **una única capa oculta** de tipo *relu* y una última capa de salida. 

El número de neuronas de la capa oculta es también uno de los aspectos más relevantes a determinar puesto que afecta tanto en la calidad del modelo como en el tiempo invertido para entrenarlo. Para el desarrollo de este modelo he realizado un estudio en el que he ido variando el número de neuronas para comprobar el comportamiento de esta arquitectura. Para entrenar esta red vamos a establecer un **mayor número de épocas** para que la red disponga de más tiempo para la convergencia. Por otro lado, **disminuimos el tamaño de los minilotes** para que se realicen un mayor número de actualizaciones de los pesos y conseguir una convergencia más constante. Finalmente también he aumentado la porción del conjunto de datos que utiliza para la validación del modelo tras cada época. A continuación observamos la tabla resultante de los experimentos realizados para determinar el número de neuronas de la capa oculta.

| Calidad | 256 | 128 | 64 | 32 | 16 |
|---|---|---|---|---|
| Accuracy | 79.78% | 80.52% | 79.78% | 80.52% | 79.03% |
| Kappa | 58.14% | 59.92% | 58.30% | 60.22% | 57.16% |

Como podemos observar, **el número de neuronas óptimo para la capa oculta es de 32** puesto que es el que aporta una mejor precisión y coeficiente de *Kappa*. Este resultado es prácticamente alcanzado por el modelo con 128 neuronas pero el coste computacional de entrenarlo es muy superior al de 32 neuronas. Con este estudio podemos determinar que a mayor número de neuronas, no siempre se obtienen mejores resultados. La razón de ser reside en que si bien un mayor número de neuronas pueden descubrir un mayor número de características, estas no siempre son útiles para resolver el problema y por ende disminuye la capacidad de predicción del clasificador. Por lo tanto, para este tercer modelo establecemos 32 neuronas en la capa oculta. A continuación se muestran la gráfica del proceso de entrenamiento y las medidas de calidad correspondientes.

```{r message=FALSE, warning=FALSE}
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_imput, 0.7)
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Nuevo modelo: 3 capas, 1 oculta
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo3 <- keras_model_sequential()
modelo3 <- modelo3 %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(ncol(datos$train) - 1)) %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo3, m_train$dataset, m_train$etiquetas, 'binary_crossentropy', 'adam', 100, 10, 0.3)
validar_modelo(modelo3, m_test$dataset, m_test$etiquetas)
```

Si comparamos los resultados con los del primer modelo, podemos apreciar en la matriz de confusión que el número de aciertos aumenta en ambas clases y el número de falsos negativos se reduce considerablemente. Por lo tanto, con esta configuración se consigue que el modelo obtenido sea capaz de diferenciar mejor las características de un pasajero que sí sobrevivió al accidente. Acompañando este hecho, disponemos de un coeficiente de *Kappa* más elevado que nos indica que este modelo está utilizando más lo que ha aprendido durante el entrenamiento que etiquetando las muestras al azar, con respecto al primer modelo, el cual solo disponía de un coeficiente de 32.69%.

Adicionalmente también he realizado varias pruebas **variando el método de optimización** [3] con el objetivo de mejorar el resultado actual. Sin embargo, todas las técnicas probadas han resultado un poco peores excepto *Nadam* (*Nesterov Adam optimizer*) cuyo clasificador disponía de una precisión y coeficiente de *Kappa* similar a *Adam*. 

Siguiendo con las modificaciones de este modelo, he estado estudiando los parámetros de esta técnica de optimización para seguir mejorando el modelo. En primer lugar he variado la **tasa de aprendizaje** dentro del intervalo [0.001, 0.1], y he podido observar que a partir de 0.05 la precisión disminuye bastante. La razón de ser reside en que a mayor tasa de aprendizaje, menos profunda es la exploración del espacio y por tanto al acelerar el aprendizaje, podemos pasar por alto buenas soluciones.
Otro de los parámetros que he probado es `AMSGrad` [4], el cual ofrece una variante del algoritmo `Adam` que parte de una tasa de aprendizaje menor y dispone de cierta memoria para normalizar la evolución del gradiente. Para nuestro problema los resultados son bastante similares por lo que no merece la pena incluir este experimento.

Por último he intentado **añadir más capas ocultas** a la topología anterior, variando el número de neuronas en cada uno, y los resultados han sido peores que la arquitectura con una sola capa oculta. Este hecho también nos demuestra que no siempre añadir un mayor número de capas mejora el modelo predictivo puesto que si el problema es relativamente sencillo, basta con una única capa de neuronas para extraer la mayoría de características y entrenar un buen clasificador, como es nuestro caso. 

## Modelo 4

```{r}
##################################
## CAPA SOFTMAX
# x_data<-m_train$dataset
# y_data<-to_categorical(m_train$etiquetas, num_classes = 2)
# library(keras)
# model = keras_model_sequential() %>%   
#   layer_dense(units = 64, activation = "relu", input_shape = ncol(x_data)) %>%
#   layer_dense(units = 64, activation = "relu") %>%
#   layer_dense(units = 64, activation = "relu") %>%
#   layer_dense(units = ncol(y_data), activation = "softmax")
# compile(model, loss = "categorical_crossentropy", optimizer = optimizer_rmsprop(), metrics = "accuracy")
# history = fit(model,  x_data, y_data, epochs = 20, batch_size = 128, validation_split = 0.2)
# # Preds
# x_data_test<-m_test$dataset
# y_data_test<-m_test$etiquetas
# y_data_pred=predict_classes(model, x_data_test)
# evaluate(model, x_data, y_data)
# 
```


# Bibliografía

[1] Documentación sobre la función mice, https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice

[2] Documentación sobre la función SMOTE, https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/SMOTE

[3] Keras Documentation, Usage of optimizers, https://keras.io/optimizers/

[4] Sashank J. Reddi, Satyen Kale & Sanjiv Kumar, ON THE CONVERGENCE OF ADAM AND BEYOND, http://www.satyenkale.com/papers/amsgrad.pdf