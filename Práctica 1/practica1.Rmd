---
title: "Práctica 1. Pre-procesamiento de datos y clasificación binaria."
author: "Lidia Sánchez Mérida"
output: pdf_document
---

# Conjuntos de datos

En esta práctica se procede a analizar el conjunto de datos procedente de *Kaggle* y denominado **IEE-CIS Fraud Detection** [1]. En él se distinguen hasta dos tipos de documentos. Aquellos que contienen el término `identity` contienen los datos personales de los individuos que han realizado transacciones bancarias. Mientras que los ficheros que contienen la palabra `transaction` disponen de la información asociada a cada una de las transacciones realizadas. Ambos ficheros son relacionables a través de un campo denominado `TransactionID`, el cual nos permite conocer más detalles acerca de las personas que han realizado las transferencias. Sin embargo, de muchas de ellas no se conoce esta información.

Para cada uno de estos tipos de ficheros existe su correspondiente conjunto de entrenamiento y validación ya separados, pero como todos los conjuntos de datos son tan amplios voy a usar directamente los ficheros `train_innerjoin.csv` y `test_innerjoin.csv` proporcionados en el repositorio de GitHub de la asignatura.

```{r message=FALSE, warning=FALSE}
# Establecemos una semilla para que los resultados sean reproducibles.
set.seed(32)
# Leemos los datos desde los ficheros
train<-read.csv(file="./train_innerjoin.csv", header=TRUE, sep=",")
test<-read.csv(file="./test_innerjoin.csv", header=TRUE, sep=",")
# Dimensiones de los conjuntos
dim(train)
dim(test)
```

Tras cargar los datos podemos observar, como anteriormente he puntualizado, que ambos cuentan con un número considerablemente amplio tanto de registros como de variables. Para conocer más información acerca de los conjuntos de datos vamos a realizar un análisis exploratorio que nos permita identificar los principales aspectos más relevantes.

# Análisis exploratorio

## Estado de los datos

Primeramente vamos a conocer el estado de los datos. Mediante la función `df_status` [2] podremos conocer los valores de todos los campos de un conjunto con el objetivo de conocer la cantidad y el porcentaje de ceros, valores nulos o infinitos. Asimismo, en la última columna también nos indica la cantidad de valores únicos que existen para cada campo. De este modo podemos conocer, por ejemplo, si una variable es categórica. Para conocer cuáles podrían ser los campos de mayor relevancia, podemos leer desde el propio Kaggle qué es lo que representa cada uno de ellos [3].

```{r message=FALSE, warning=FALSE, include=FALSE}
# Obtenemos el estado de los dos conjuntos de transacciones
library(funModeling)
train_st<-df_status(train)
test_st<-df_status(test)
```

Si bien el resultado proporcionado por estas funciones es súmamente extenso como para imprimirlo, he realizado un análisis acerca de las variables más relevantes obteniendo las siguientes conclusiones:

* En primer lugar destacamos que la variable categórica `isFraud` que intentamos predecir dispone de un altísimo número de ejemplos de transacciones no fraudulentas en el conjunto de entrenamiento. Tal es así que apenas existe un 8% de transacciones clasificadas como fraudulentas. Si bien este tipo de fenómenos es bastante común, clases tan súmamente desbalanceadas suelen dificultar el proceso de entrenamiento y obtención de buenos clasificadores.
* Casi la mitad de las variables disponen de aproximádamente un 50% de valores perdidos. Mayoritariamente se encuentran relacionadas con medidas tales como la distancia, características como la dirección, entre otras.
* Existen bastantes variables categóricas, además de `isFraud` que indican los dominios origen y destino, el tipo de dispositivo desde el que se hizo la transacción, el producto de la misma, entre otras.

## Relación dinero-fraude

A continuación procedo a representar gráficamente otro tipo de estadísticas que pueden también ser interesantes para conocer, un poco, las características del tipo de transacciones. En primer lugar vamos a averiguar si existe algún tipo de **relación entre la cantidad de la transacción y su clasificación como fraudulenta o no**. El objetivo es conocer si existe un patrón para detectar las transacciones fraudulentas, como por ejemplo si en muchas de ellas se han traspasado grandes cantidades de dinero. Para ello vamos a dibujar un histograma para representar la variable `TransactionAmt`, que es la que contiene las sumas de dinero, y las diferenciaremos en función de si son o no fraudulentas. Para este último paso deberemos de transformar los valores de la columna `isFraud` a categóricos  

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
# Transformamos 0->No, 1->Yes para diferenciar las transacciones
# fraudulentas de las que no lo son.
train_categ<-train %>% mutate(isFraud = as.factor(ifelse(isFraud == 1, 'Yes', 'No')))
# Representamos las cantidades y si son de transacciones fraudulentas o no.
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionAmt),
     fill = as.factor(isFraud)), binwidth = 100)
```

Como se puede observar no existen grandes diferencias entre las cantidades monetarias de las transacciones clasificadas como fraudulentas de las que no lo son. Sin embargo, este histograma nos permite comprobar que la **mayoría de transacciones se encuentran agrupadas** dentro de un rango aproximado [0-700], lo cual nos posibilita realizar un **corte horizontal para reducir el número de registros** a los contenidos en dicho rango.

## Relación tiempo-fraude

Aprovechando los dos conjuntos separados de transacciones, vamos a estudiar como segundo caso la **relación entre el tipo de transacción y la medida de tiempo asociada** en el campo `TransactionDT`. Esta representa la diferencia entre dos valores temporales, y aunque si bien en la descripción del dataset no especifican las medidas utilizadas, sus valores se basan en los de los campos DX donde X es un número entre 1 y 15. De estos conocemos por la descripción en Kaggle que se consideran, por ejemplos, los días transcurridos entre una transacción y otra. El objetivo de este análisis consiste en averiguar si el rango temporal entre dos transacciones puede ser un factor a considerar para averiguar si son o no fraudulentas.

```{r}
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionDT),
    fill = as.factor(isFraud)), binwidth = 1000000)
```

Tal y como se puede observar, aquellas transacciones marcadas como **fraudulentas disponen de un valor menor que las no fraudulentas**, por lo que podemos considerar que quizás este factor de tiempo puede estar relacionado con la naturaleza legal de la transacción. Una posible teoría que explique este suceso puede basarse en que cuando, por ejemplo hacemos algo que está mal, intentamos que sea lo más rápidamente posible. En análisis posteriores comprobaremos si efectívamente existe algún tipo de relación entre esta variable y la columna a predecir que nos ayude a entrenar modelos predictivos.

# Preprocesamiento de datos

En esta sección procedemos a aplicar diversas técnicas para mejorar la calidad del conjunto de datos de modo que podamos obtener un subconjunto más sencillo con el que entrenar los futuros modelos predictores. Uno de los principales problemas de este conjunto reside en su **gran dimensionalidad** tanto del número de registros como de columnas. El hecho de disponer de un enorme grupo de muestras puede provocar problemas de rendimiento a la hora de intentar entrenar un modelo con tal volumen de datos. Mientras que disponer de tal cantidad de variables también puede producir grandes tiempos de espera en entrenamiento al considerar tantas columnas para obtener el clasificador. Estos son los dos principales problemas que se abordan a continuación.

## Reducción del número de muestras aleatoriamente

En primer lugar debo reducir el número de registros del conjunto de datos para poder trabajar con él. Para ello, en primer lugar, vamos a realizar el corte horizontal hablado anteriormente y referente al **análisis exploratorio de la cantidades monetarias** de las transacciones. De este modo obtendremos un conjunto de datos más reducido y orientado hacia el grupo mayoritario de transacciones. Para ello vamos a eliminar aquellos registros con un cantidad superior a 700 USD.

```{r}
library(dplyr)
library(magrittr)
minitrain_700<-train %>% filter(TransactionAmt < 700)
cat("Dimensiones al quitar transacciones > 700 USD\n")
dim(minitrain_700)
```

Tal y como podemos comprobar, hemos conseguido reducir casi 1.000 registros. Al sufrir tan poca variación realmente **no creo que merezca la pena eliminar 1.000 muestras** del conjunto puesto que este hecho no supone un cambio significativo en el entrenamiento de los modelos. Asimismo, dependiendo de las combinaciones de procesamientos que se hagan del conjunto para entrenar los modelos, se evaluará la posibilidad de reducir el número de registros en caso de que las técnicas utilizadas tarden mucho tiempo en devolvernos un clasificador. 

## Variables con demasiados valores perdidos

Comenzamos abordando otro de los principales problemas de este conjunto de datos que consiste en la aparición numerosa de valores perdidos o NAs. Si bien uno de los métodos posibles para eliminarlos es predecirlos utilizando alguna técnica que nos permita obtener un valor aproximado a partir de la información que existe en el conjunto, si el número de estos valores es sumamente predominante **(> 50%)** es prácticamente imposible aplicar este método. Por lo tanto, a continuación se desarrolla una función en la que se eliminarán aquellas columnas del dataset que dispongan de un número de valores perdidos superior al 50% de los datos totales. Para ello me he basado en los ejemplos de los *scripts* proporcionados en la asignatura.

```{r message=FALSE, warning=FALSE, include=FALSE}
eliminar_columnas_nas<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(funModeling)
  estado<-df_status(datos)
  # Obtenemos aquellas columnas con más de un 50% de valores perdidos (NAs) a partir del estado
  na_cols <- estado %>%
    filter(p_na > 50) %>%
    select(variable)
  # Eliminamos las columnas obtenidas
  remove_cols <- bind_rows(
    list(na_cols)
  )
  # Obtenemos el conjunto de datos reducido 
  datos_finales <- datos %>%
    select(-one_of(remove_cols$variable))
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Desviación estándar de las variables

Se consideran variables irrelevantes aquellas que no aportan información útil debido a que sus **valores son muy similares**. Este tipo de columnas no nos ayudan a predecir, posteriormente, si una transacción es o no fraudulenta puesto que independientemente de las muestras su comportamiento no varía. Uno de los métodos por los que se puede decidir si una variable dispone de valores interesantes es mediante la **desviación estándar**. Si esta medida estadística es 0 entonces nos indica que no existe variabilidad en los valores de la columna, y por tanto, esta variable no será útil. 

Para realizar este preprocesamiento vamos a hacer uso de la función `nearZeroVar` [4] que calcula el número de valores únicos y su frecuencia de aparición. De este modo obtendremos aquellas variables cuya desviación estándar sea 0 o cercana a dicho valor. Previo a su aplicación, es necesario transformar todas las variables a numéricas para poder calcular la desviación estándar. Para las columnas categóricas se asignará a cada etiqueta un único valor que la represente.

```{r message=FALSE, warning=FALSE, include=FALSE}
eliminar_columnas_sd<-function(datos, eliminar_todas) {
  # Convertimos todas las variables a variables numéricas
  # escepto isFraud porque cambia 0->1 y 1->2
  for(i in c(1:ncol(datos))) {
    if (names(datos[i]) != "isFraud") {
      datos[,i] <- as.numeric(datos[,i])
    }
  }
  # Calculamos el número de variables con desviación estándar cercana o igual a 0.
  library(caret)
  nz<-nearZeroVar(datos, names = TRUE, saveMetrics = TRUE)
  # Desviación estándar = 0
  nz0<-nz[nz[,"zeroVar"] > 0, ]
  # Desviación estándar cercana a 0
  nz_cercano0<-nz[nz[,"zeroVar"] + nz[,"nzv"] > 0, ]
  # Eliminamos las variables con desviación estándar = 0
  datos_finales<-datos[, -which(colnames(datos) %in% rownames(nz0))]

  # Si eliminar_todas=TRUE entonces también eliminamos las variables con sd cercana a 0
  if (eliminar_todas == TRUE) {
    datos_finales<-datos_finales[, -which(colnames(datos_finales) %in% rownames(nz_cercano0))]
  }
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Tratamiento de valores perdidos

En este apartado procedemos a tratar los numerosos valores perdidos que contiene el conjunto de datos. Si bien existen multitud de técnicas para aplicarle a este tipo de datos, yo voy a considerar las siguientes:

* **Eliminación de NAs**. Esta puede ser la técnica más drástica para tratar los valores perdidos. Para llevarla a cabo vamos a aplicar la función `na.omit()` que eliminará todos los registros que contentan este tipo de valores.
* **Imputación de NAs**. Al contrario que la anterior, esta técnica más sofisticada es una de las más utilizadas según he estado investigando. Para aplicarla existen varias librerías pero al parecer la más popular es `mice` debido a las múltiples operaciones que puede llevar a cabo para asignar un valor a cada NA [5]. Dependiendo de la naturaleza de los datos, existen métodos específicos aunque también dispone de métodos generales aplicables a cualquier tipo de dato.

A continuación se expone el código de las funciones asociadas a las dos técnicas explicadas anteriormente.

```{r warning=TRUE, warning=FALSE, include=FALSE}
eliminar_nas<-function(datos) {
  datos_finales<-na.omit(datos)
  # Devolvemos el conjunto resultante
  datos_finales
}

imputar_nas<-function(datos, imputs, iters, metodo) {
  library(mice)
  # Especificamos el número de imputaciones, el número de iteraciones
  # por imputación y el método a utilizar para imputar los NAs
  modelo_mice<-mice(datos, m=imputs, maxit=iters, method=metodo)
  # Obtenemos el conjunto de datos imputado
  datos_imputados<-complete(modelo_mice)
  # Devolvemos el conjunto resultante
  datos_imputados
}
```

## Correlación

En esta sección vamos a estudiar la **correlación existente entre las variables y la columna a predecir**, que en el caso de este *dataset* es `isFraud`, y posteriormente analizaremos cuán **relacionadas se encuentran las variables entre sí**. El objetivo es, por un lado, conocer si existe alguna variable o grupo de variables que expliquen el comportamiento de la variable categórica a predecir. Mientras que por otro lado queremos conocer si existen variables muy correladas entre sí de manera que podamos reducir la dimensionalidad eliminando aquellas cuyo coeficiente de correlación sea muy alto, ya que este sería un indicio de que dichas columnas aportan la misma información.

Para el primer estudio vamos a aplicar la función `correlation_table` tal y como se ejemplifica en uno de los *scripts* proporcionados en la asignatura. Como precondición para calcular los coeficientes de correlación, es necesario que las columnas dispongan solo de valores numéricos. 
De igual forma, esta condición es necesaria para el segundo análisis que involucra a todas las variables, para el cual usaremos, en primer lugar, la función `cor` que se encargará de calcular los coeficieintes de correlación entre todas las variables del conjunto. Una vez dispongamos de esta matriz, aplicaremos la función `findCorrelation` [6] que nos ayudará a identificar aquellas variables que tienen un coeficiente de correlación especificado como argumento. De este modo se puede personalizar el nivel de exigencia con el que eliminar las columnas correladas.

A continuación se presentan las funciones asociadas a los dos estudios de correlación que se realizarán a posteriori.

```{r message=FALSE, warning=FALSE, include=FALSE}
correlacion_fraude<-function(datos) {
  library(funModeling)
  # Calculamos los coeficientes de correlación en torno a la variable clasificatoria
  tabla_corr<-correlation_table(datos, target='isFraud')
  tabla_corr
}

correlacion_variables<-function(datos, max_corr) {
  library(caret)
  # Calculamos los coeficientes de correlación todas-todas las variables.
  coefs<-cor(datos)
  # Excluimos las filas con valores perdidos porque si no esta función no se puede aplicar.
  cols_corr<-findCorrelation(na.omit(coefs), cutoff=max_corr)
  # Eliminamos las columnas resultantes del dataset
  datos_finales<-datos[-c(cols_corr)]
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Outliers

Los *outliers* son datos cuyos valores están caracterizados por encontrarse fuera del rango normalizado para el atributo. En otras palabras, este tipo de datos disponen de valores muy diferentes a los del resto del conjunto y por ende pueden afectar a la capacidad de generalización de los modelos entrenados. Por este motivo, esta sección se va a dedicar al estudio y tratamiento de este tipo de valores. Para el análisis me he inspirado en este ejemplo [7] en el que se hace uso de la función `boxplot` [8], con la que se calculan los valores situados fuera del rango particular de cada variable. De este modo podremos conocer los *outliers* de cada una de las columnas.

```{r message=FALSE, warning=FALSE}
estudio_outliers<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Calculamos los outliers de todas las variables del conjunto de datos.
  outliers<-datos %>%
        map(~ boxplot.stats(.x)$out) 
  # Devolvemos los resultados
  outliers
}
```

Una vez disponemos de información acerca de los *outliers* existentes en el conjunto de datos, procedemos a estudiar los diferentes tratamientos que se les pueden aplicar. Dentro del ejemplo mecionado anteriormente se presentan varias técnicas, de las cuales he escogido las dos más interesantes a mi parecer: **la imputación mediante una determinada fórmula y la predicción de los valores**. La primera técnica consiste en reemplazar aquellos valores que se sitúen fuera del rango de los cuantiles 25 y 75, por el cuantil 5 o el 95, respectivamente. El objetivo es transformar los *outliers* que se encuentran por debajo del rango en el valor más pequeño que se encuentra dentro de dicho intervalo, y realizar el mismo procedimiento con los que se encuentran por encima del rango pero, en este caso, asignando el mayor valor posible.

La segunda técnica consiste en convertir todos los *outliers* detectados en valores NA para posteriormente imputarlos. Como ya disponemos de una función que se encarga de tratar los valores perdidos, será esa que reutilizaremos para predecir este tipo de valores y así probar esta segunda técnica. A continuación se presentan las funciones asociadas a los dos tratamientos explicados anteriormente.

```{r message=FALSE, warning=FALSE, include=FALSE}
outliers_formula<-function(datos) {
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_finales<-datos[, -which(names(datos) %in% c("isFraud"))]
  i <- 1
  for(col in datos_finales) {
    # Calculamos los cuantiles 25 y 75 para comprobar es un outlier
    quantiles<-quantile(col, probs=c(0.25, 0.75))
    # Calculamos los cuantiles 5 y 95 para asociar como nuevos valores a los outliers
    nuevos_valores<-quantile(col, probs=c(0.05, 0.95))
    # Calculamos la varianza máxima que puede sufrir un valor
    H<-1.5*IQR(col)
    # Sustitución de outliers
    col[col < (quantiles[1] - H)] <- nuevos_valores[1]
    col[col > (quantiles[2] + H)] <- nuevos_valores[2]
    # Actualizamos el dataset
    datos_finales[i]<-col
    i <- i + 1
  }
  # Volvemos a añadir la columna `isFraud`
  datos_finales<-cbind(datos_finales, isFraud=datos$isFraud)
  # Devolvemos los datos resultantes
  datos_finales
}

outliers_prediccion<-function(datos, imputs, iters, metodo) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_sin_fraud<-datos[, -which(names(datos) %in% c("isFraud"))]
  # Transformamos los valores outliers en valores perdidos (NAs)
  datos_nas<-as.data.frame(apply(datos_sin_fraud, 2, function(row){row[row %in% boxplot(row, plot = FALSE)$out] = NA; row})) 
  datos_finales<-imputar_nas(datos_nas, imputs, iters, metodo)
  # Devolvemos el dataset resultante
  datos_finales
}
```

## Selección de variables

El objetivo de este apartado consiste en reducir la dimensionalidad del *dataset* en lo referente a las variables que nos permiten predecir la columna categórica `isFraud`. Si bien existen diferentes técnicas para llevar este procedimiento a cabo, vamos a utilizar una de las más populares denominada **Análisis de Componentes Principales (PCA)**. Su procedimiento consiste en realizar diversas combinaciones lineales de los atributos del dataset de modo que nos indique cuáles son los más relevantes a considerar. Este tipo de técnicas se deben aplicar solo cuando el número de variables es considerablemente amplio, puesto que si lo llevamos a cabo con un número de columnas razonable podemos perder información al combinarlas. 

Si bien existen dos paquetes principales para aplicar el PCA con y sin valores perdidos, vamos a hacer uso de este último puesto que una de las precondiciones de la función es que el *dataset* se encuentre preprocesado e imputado. Asimismo, cabe destacar que este algoritmo es altamente sensible a las distancias entre los valores de las columnas, como datos muy grandes. Por ello es recomendable realizar un **centrado y escalado** de los datos previa a la aplicación del algoritmo. Para realizar todo el procedimiento voy a hacer uso de la función `preProcess` [9] la cual aplica tanto el escalado y centrado como el algoritmo PCA a todo el conjunto de datos exceptuando la variable a predecir. 
Asimismo, también se le puede especificar el **porcentaje de varianza máximo** deseado para seleccionar las variables que la representen. A menor valor, menor número de variables pero también peor será la representación de estas variables del conjunto de datos. Por último obtenemos el *dataset* resultante con las combinaciones de variables obtenidas por el algoritmo con la función `predict`.

```{r}
pca<-function(datos, umbral) {
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_sin_fraud<-datos[, -which(names(datos) %in% c("isFraud"))]
  library(caret)
  # Centrado, escalado y PCA
  pca<-preProcess(datos_sin_fraud, method = c("center", "scale", "pca"), thres=umbral)
  # Obtenemos el dataset con las combinaciones de las variables obtenidas del PCA
  datos_pca<-predict(pca, datos_sin_fraud) 
  # Volvemos a añadir la columna `isFraud`
  datos_pca<-cbind(datos_pca, isFraud=datos$isFraud)
  # Devolvemos el conjunto de datos resultante
  datos_pca
}
```

## Balanceo de la clase a predecir

En esta última sección se tratará el desbalanceo de la clase `isFraud` con la que debemos entrenar los modelos para poder predecir si una transacción es fraudulenta o no. Como hemos podido observar en el análisis exploratorio, la gran mayoría de muestras son transacciones no fraudulentas (cerca del 96%) mientras que la clase minoritaria es la asociada a las transacciones fraudulentas. Este tipo de problemas es bastante común, pero en el caso de este dataset **es súmamente desproporcionado**. Para intentar paliar este inconveniente se van a aplicar dos de las técnicas más populares: **undersampling y oversampling**. En el primer caso se obtendrá un subconjunto de datos en el que se iguale el número de muestras de ambas clases, es decir, se escogerán tantas muestras de transacciones no fraudulentas al azar como ejemplos de transacciones fradulentas haya [downsample]. Con esta técnica el conjunto de datos se reducirá drásticamente, lo cual provocará una serie de consecuencias que analizaremos posteriormente al entrenar los modelos.

```{r}
under_sampling<-function(datos) {
  library(caret)
  # Obtenemos las etiquetas a predecir
  etiquetas<-factor(datos$isFraud)
  # Under-sampling para igualar ambas clases
  set.seed(32)
  datos_finales<-downSample(datos[, -which(names(datos) %in% c("isFraud"))],
      etiquetas, yname='isFraud')
  # Devolvemos los datos resultantes
  datos_finales
}
```

## División del conjunto de datos

Por último vamos a introducir una función cuyo objetivo consiste en dividir un conjunto de datos en **entrenamiento y validación**. La razón de ser reside en la posibilidad de poder calcular un mayor número de medidas de calidad tras validar los modelos con el conjunto de test más que considerar solamente la tasa de error que nos devolvería Kaggle si subimos los datos. Para ello vamos a hacer uso de la función `createDataPartition`[] a la cual se le proporciona el conjunto de datos a dividir especificando la variable a predecir, el número de muestras para el conjunto de entrenamiento y el número de particiones.

```{r}
get_train_test<-function(datos, porcentaje_train) {
  # Mezclamos los datos aleatoriamente
  set.seed(32)
  datos<-datos[sample(1:nrow(datos)), ]
  # Dividimos el conjunto en train y test
  library(caret)
  train<-createDataPartition(datos$isFraud, p=porcentaje_train, list=FALSE, times=1)
  datos_train<-datos[train, ]
  datos_test<-datos[-train, ]
  # Devolvemos ambos dataframes en una lista
  df<-list()
  df$train<-datos_train
  df$test<-datos_test
  df
}
```

# Modelos

Esta sección se encuentra orientada a producir diversos modelos predictivos utilizando varias combinaciones de las técnicas de preprocesamiento explicadas anteriormente. El objetivo es estudiar si la capacidad de generalización de los modelos mejora o empeora en función de los diferentes cambios de preprocesamiento que introduzcamos. Para todas las combinaciones vamos a aplicar los mismos métodos de entrenamiento para que los modelos obtenidos sean comparables y así basar el análisis en función de las técnicas de preprocesamiento de datos. Para ello voy a utilizar, principalmente, la librería `e1071` con el objetivo de entrenar modelos utilizando la técnica **Naive Bayes**, la cual se caracteriza por su rapidez y por estar orientada a la clasificación binaria. Como segunda técnica utilizaré **Random Forest** por ser más robusta y sofisticada que la anterior con el fin de intentar obtener un modelo más refinado que sea capaz de proporcionar mejores resultados. Para ello haré uso de la librería de igual nombre `randomForest`.

A continuación se presentan dos funciones con las que calcular las predicciones realizadas dado un modelo así como su matriz de confusión y toda la información asociada a la calidad del mismo. Por otro lado también se incluye una segunda función para dibujar la curva ROC y calcular el área debajo de esta para poder comprobar visualmente la capacidad de generalización de un modelo proporcionado. Se proponen como funciones para poder utilizarlas en todos los modelos que se entrenen en esta sección.

```{r}
calidad_modelo<-function(modelo, test, etiquetas_test) {
  # Obtenemos las predicciones del modelo para el conjunto de test
  predicciones<-predict(modelo, test)
  # Obtenemos la matriz de confusión y la información 
  confusion<-table(etiquetas_test, predicciones, dnn = c("Real", "Predicha"))
  resultado<-list()
  resultado$preds<-predicciones
  resultado$conf<-confusion
  resultado
}

curva_roc<-function(predicciones, etiquetas) {
  library("ROCR")
  # Calculamos el porcentaje de acierto entre las etiquetas predichas y las reales
  preds<-prediction(as.numeric(predicciones), as.numeric(etiquetas))
  # Comprobamos la eficacia del modelo considerando los falsos positivos y los aciertos
  curva<-performance(preds, "tpr", "fpr")
  # Dibujamos la curva.
  plot(curva, col="green", add=FALSE, main="Curva ROC", lwd = 2) 
  segments(0, 0, 1, 1, col='black')
  grid() 
  # Calculamos el área debajo de la curva
  curva.area = performance(preds, "auc")
  cat("\nEl área bajo la curva ROC es", curva.area@y.values[[1]]*100,"%\n")
}
```

## Modelos del primer preprocesamiento

En este primer modelo vamos a comenzar eliminando aquellas columnas con más de un **50% de valores perdidos**. De este modo conseguimos reducir el número de variables de **434 a 305**, que si bien siguen siendo muchas solo eliminando las columnas con más de un 50% de NAs hemos conseguido una buena reducción del número de columnas. A continuación, procedemos a eliminar las columnas con **desviación estándar=0** para continuar reduciendo la dimensionalidad del conjunto de modo que, añadiendo este segundo procesamiento, hemos conseguido reducir hasta las **301 columnas**.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Eliminamos columnas con > 50% NA
train_m1<-eliminar_columnas_nas(train)
# Eliminamos columnas con sd=0
train_m1<-eliminar_columnas_sd(train_m1, FALSE)
```

Una vez hemos reducido casi la mitad de las columnas del *dataset*, previo a realizar estudios más sofisticados vamos a **tratar los valores perdidos**. Para ello vamos a aplicar la primera técnica explicada anteriormente consistente en **eliminar todos los registros con NA**. De este modo podemos comprobar cómo el conjunto de datos se ve grávemente afectado puesto que pasa de más de 100.000 muestras a disponer solo de aproximadamente **21.984 ejemplos**. Como ya anticipamos este método es muy agresivo pero cuando entrenemos los modelos comprobaremos si este hecho se ve reflejado en su calidad.

Aplicando algunas técnicas para conocer la importancia de las 301 variables que disponemos me he dado cuenta de que al eliminar los valores perdidos algunas variables **vuelven a tener una desviación estándar igual a 0**. Por lo tanto vamos a realizar una segunda pasada de esta función para eliminarlas con el objetivo de poder, a continuación, estudiar la correlación entre variables y con la variable a predecir para continuar reduciendo la dimensionalidad asociada a las columnas. Tal y como comprobar han sido un total de cuatro variables las que han sufrido la desviación estándar 0 al eliminar los valores NA, por lo que deducimos que estas columnas no han sido eliminadas anteriormente por contar con este tipo de valores que les sumaban cierta variabilidad a su rango. Por lo tanto en este momento contamos con **297 variables**.

```{r}
# Eliminar registros con valores NA
train_m1<-eliminar_nas(train_m1)
# Volvemos a eliminar las nuevas columnas que han surgido con desviación estándar=0
# tras quitar valores perdidos
train_m1<-eliminar_columnas_sd(train_m1, FALSE)
# Dimensión del conjunto resultante
cat("\nSegunda tanda para eliminar variables con sd=0\n")
dim(train_m1)

# Correlación entre variables
train_m1<-correlacion_variables(train_m1, 0.75)
# Dimensión del conjunto resultante
cat("\nTras eliminar variables correladas > 0.75\n")
dim(train_m1)

# Correlación entre las variables y la categórica `isFraud`
tabla_corr<-correlacion_fraude(train_m1)
head(tabla_corr, 10)
# Obtenemos las variables que tienen un coeficiente de correlación muy bajo con respecto a isFraud
tabla_corr_baja<-tabla_corr %>% filter(isFraud > -0.1 & isFraud < 0.1)
# Las eliminamos del conjunto de datos haciendo una copia en otro para no perder el original
train_m1_corr<-train_m1[, -which(colnames(train_m1) %in% tabla_corr_baja$Variable)]
# Dimensión del conjunto resultante
cat("\nTras eliminar variables con correlación en [-0.1, 0.1] con respecto a isFraud\n")
dim(train_m1_corr)
```

A continuación vamos a estudiar la **correlación entre variables**. Como se ha explicado al presentar su correspondiente función, nuestro objetivo es eliminar aquellas que proporcionen información redundante para así reducir más el número de columnas del *dataset*. Para ello vamos a establecer un umbral de **0.75** por el cual eliminaremos todas aquellas variables cuyos coeficientes sean mayores a dicho valor puesto que este conlleva un indicio de alta correlación entre columnas. Este procesamiento nos ha hecho bajar de 286 variables a solo **84 columnas**, por lo que podemos concluir que la gran mayoría de variables restantes proporcionaban la misma información y ahora vamos a considerar aquellas que son más relevantes.

En relación al análisis de **correlación entre las variables y la categórica `isFraud`** podemos visualizar los diez primeros resultados puesto que la tabla es considerablemente amplia como para mostrarla entera. Sin embargo, estudiándola en profundidad he podido percatarme de que existen muchas columnas con menos de 0.1 de correlación. Si bien en este caso el objetivo es el mismo de reducir el número de columnas del que disponemos, en este caso queremos disponer de aquellas **variables que tengan un mayor coeficiente de correlación con `isFraud`** puesto que este hecho nos indica que dichas columnas pueden explicar el comportamiento de la variable a predecir. Mientras que si bien las columnas con una baja correlación por sí solas no ayudan a predecir la variable categórica, sí que pueden aportan información valiosa si se combinan con otras columnas. Para comprobar esta teoría vamos a entrenar unos primeros modelos **eliminando aquellas variables con una correlación en el intervalo [-0.1, 0.1]** de modo que excluyamos todas aquellas columnas con una relación muy baja con la variable `isFraud` en base al coeficiente de correlación. Con este procesamiento ya disponemos de **34 variables**, un número mucho más manejable de columnas del que teníamos anteriormente.

A continuación vamos a realizar un estudio de los **outliers** para comprobar de cuántos de estos valores disponen las 34 columnas del conjunto de datos resultante hasta el momento. De nuevo mostramos solo los 10 primeros resultados porque aun siendo menos columnas, la lista de *outliers* es bastante extensa para mostrarla al completo. Visualizándola en profunidad he podido observar que la gran mayoría de columnas disponen de este tipo de valores en mayor o menor cantidad, por ello se le va a aplicar el primero de los tratamientos explicados, consistente en transformarlos en el cuantil 5 o 95 en función de si se encuentran por debajo o por encima del rango de valores, respectivamente.

```{r message=FALSE, warning=FALSE}
# Estudio de los outliers
cat("\nOutliers antes del tratamiento\n")
head(summary(estudio_outliers(train_m1_corr)), 10)
# Tratamiento de outliers mediante la fórmula
train_m1_out<-outliers_formula(train_m1_corr)
# Estudio del conjunto resultante
cat("\nOutliers tras el tratamiento\n")
head(summary(estudio_outliers(train_m1_out)), 10)
```

Una vez hemos tratado los *outliers* de todas las columnas excepto `isFraud` volvemos a realizar un análisis para comprobar si aún existen este tipo de valores. Y tal y como podemos observar así es, algunas variables siguen disponiendo de este tipo de valores. La razón de ello reside en que si un valor *outlier* no supera el umbral que se calcula para la variable, no se modifica y por lo tanto pasa al nuevo conjunto resultante. Por el momento vamos a mantener este conjunto de datos para posteriormente comparar los modelos entrenados con él y con otro conjunto al que le apliquemos la segunda técnica explicada para los *outliers*.

Como ya disponemos de un conjunto de variables más reducido, **no vamos a aplicar el PCA** puesto que nos arriesgamos a perder información a la hora de combinar las columnas. Por lo que procedemos directamente a realizar el último paso previo al entrenamiento de los modelos: **balancear la clase a predecir**. Para ello vamos a aplicar la primera técnica explicada denominada **under-sampling**, con el objetivo de disponer del mismo número de transacciones fraudulentas como de no fraudulentas. Tal y como podemos observar el conjunto se ha reducido considerablemente puesto que el número de muestras fraudulentas es súmamente menor que el de no fraudulentas, y si a esto le sumamos partir de un conjunto con no demasiadas muestras pues al final el *dataset* dispone de pocos ejemplos.

Una vez disponemos del conjunto de datos preprocesado, lo dividimos en un conjunto de entrenamiento y test para validar los modelos que vamos a entrenar a continuación con el objetivo de calcular un mayor número de medidas que la tasa de error que solo nos devuelve Kaggle. Para ello vamos a separar el **70% de los datos para entrenamiento y el 30% dedicado a la validación de los modelos**. Si bien la práctica de aplicar el preprocesamiento sobre el conjunto de datos completo, afectando también al futuro conjunto de validación, no es técnicamente correcto como facilita súmamente el trabajo es muy utilizada y por lo tanto yo también voy a trabajar de esta forma. Por lo tanto voy a obtener sendos conjuntos de entrenamiento y validación de los datos ya preprocesados.

```{r message=FALSE, warning=FALSE}
# Under-sampling para balancear la clase `isFraud`
train_m1_final<-under_sampling(train_m1_out)
cat("Conjunto de datos tras under-sampling\n")
dim(train_m1_final)
# Dividimos el conjunto final en 70% entrenamiento y 30% test
datos<-get_train_test(train_m1_final, 0.7)
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nConjunto de test\n")
dim(datos$test)
cat("\n")

# MODELO NAIVE BAYES
set.seed(32)
library(e1071)
# Entrenamos el modelo utilizando todas las características
modelo_nb<-naiveBayes(isFraud~., data = datos$train)
# Obtenemos las predicciones y matriz de confusión
calidad_nb<-calidad_modelo(modelo_nb, datos$test, datos$test$isFraud)
confusionMatrix(calidad_nb$conf, positive='1')
```

Como podemos observar hemos utilizado la función `confusionMatrix` [9] para obtener más información acerca del modelo a partir de la propia matriz de confusión. Si bien la documentación oficial de la función no aporta información acerca de los atributos mostrados, he encontrado un ejemplo [10] en el que se detallan cada uno de ellos. En primer lugar destacamos que la **precisión del modelo es de un 74%**, valor que se encuentra dentro de un intervalo al 95%, lo que nos asegura casi con totalidad que la precisión del modelo mostrada es verídica. Por otro lado podemos observar en el atributo *No Information Rate* cómo la función ha detectado el balanceado de la clase `isFraud` puesto que este valor indica que no existe una mayor representación de una clase y por ende el clasificador no se ve influido por una clase mayoritaria.

Si nos fijamos en un campo denominado *Kappa* podemos conocer cuán bueno es el clasificador entrenado frente a otro que escoge las etiquetas al azar. El objetivo es conocer si el clasificador obtenido, en este caso por Naive Bayes, realmente está prediciendo las etiquetas o las clasifica más aleatoriamente. Investigando más acerca de la repercusión de este valor [12] se considera que un modelo es confiable cuando este valor se encuentra **por encima del 80%**, lo que significa que apenas presenta cierta aleatoriedad en sus clasificaciones. En este caso podemos apreciar que nuestro valor es del 47%, lo que significa que en más de **la mitad de las etiquetas coincide con un clasificador aleatorio**. Tal y como se explica en la fuente [12], la gravedad de estos resultados dependen del ámbito en el que se encuentre y considerando la capacidad de detectar transacciones fraudulentas, podemos determinar que **este clasificador no es para nada competitivo** dentro de un ámbito real.

```{r message=FALSE, warning=FALSE}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_nb$preds, datos$test$isFraud)
```

Visualizando la curva ROC podemos comprobar la teoría anterior puesto que tal y como podemos observar el clasificador no dispone de una gran capacidad de generalización puesto que su área apenas llega a un 74%. 

A continuación vamos a repetir el mismo proceso pero utilizando **Random Forest** y los mismos conjuntos de datos de entrenamiento y validación. 

```{r message=FALSE, warning=FALSE}
# MODELO RANDOM FOREST
set.seed(32)
library(randomForest)
# Entrenamos el modelo utilizando todas las características
modelo_rf<-randomForest(formula=isFraud~., data = datos$train, ntree=100)
# Obtenemos las predicciones y matriz de confusión
calidad_rf<-calidad_modelo(modelo_rf, datos$test, datos$test$isFraud)
confusionMatrix(calidad_rf$conf, positive='1')
```

Tal y como podemos observar este modelo cuenta con una **precisión del 83%**, 9 puntos más que el modelo anterior. Si observamos, además, la matriz de confusión podemos apreciar una disminución de los falsos positivos lo que acompaña a esta mejora en la tasa de aciertos. Sin embargo, analizando de nuevo el resto de atributos según [11] en este caso el campo *Kappa* muestra aproximadamente un **67% de acierto** con respecto al clasificador entrenado por el método en cuestión. Si bien sigue sin llegar al recomendable 80%, podemos afirmar que este modelo muestra una mayor confiabilidad de cara a utilizarlo en un ámbito real comparado con el anterior puesto que sus resultados coinciden menos con los de un clasificador aleatorio. Aún así, con **este preprocesamiento ninguno de los modelos entrenados son competitivos**. 

```{r message=FALSE, warning=FALSE}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_rf$preds, datos$test$isFraud)
```

Si además visualizamos la curva ROC del modelo entrenado con Random Forest podemos apreciar una mayor amplitud, lo que produce un **área mayor bajo la curva**, por lo que bajo el mismo preprocesamiento de los conjuntos esta técnica ha conseguido un clasificador con una mejor capacidad de generalización frente a la técnica Naive Bayes utilizada para el primer modelo.

## Modelos del segundo preprocesamiento

En este segundo preprocesamiento vamos a partir del conjunto de datos de entrenamiento original. El primer paso que vamos a realizar es **balancear la clase `isFraud`** y así también reducir la dimensionalidad del conjunto. El objetivo consiste en preprocesar los datos de modo que no exista una clase mayoritaria y así  aplicar las siguientes técnicas sobre un conjunto de datos ya balanceado.

```{r message=FALSE, warning=FALSE}
# Balanceamos las clases tomando todas las transacciones
# positivas y el mismo número de transacciones negativas
train_m2<-under_sampling(train)
# Comprobamos que ambas clases se encuentran balanceadas
cat("\nTransacciones NO fraudulentas (clase 0)\n")
length(train_m2$isFraud[train_m2$isFraud==0])
cat("\nTransacciones SI fraudulentas (clase 1)\n")
length(train_m2$isFraud[train_m2$isFraud==1])
```

Como podemos observar hemos pasado de tener más de 100.000 registros a solo disponer de 11.318 en cada clase lo que hace un total de **22.636 muestras**. Una vez disponemos del conjunto de datos balanceado, comenzamos a eliminar las columnas con más de un 50% de NAs y aquellas con desviación estándar igual a 0, como hicimos en el primer procesamiento.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Eliminamos las columnas con > 50% NAs
train_m2<-eliminar_columnas_nas(train_m2)
# Eliminamos las columnas con sd=0
train_m2<-eliminar_columnas_sd(train_m2, FALSE)
```

Tal y como se puede comprobar hemos pasado de las 434 variables originales a **324 columnas**. A continuación, vamos a introducir un segundo cambio en el preprocesamiento anterior de modo que **no eliminaremos los valores perdidos o NAs, sino que los imputaremos utilizando la función `mice`**. Con ella se puede especificar el método a utilizar para imputar los valores perdidos así como el número de imputaciones e iteraciones por cada una de ellas. En relación al primer parámetro he probado en primer lugar *rf* (*Random Forest*), pero debido a la poca variabilidad de los datos esta técnica no ha producido buenos resultados. Otro método sofisticado es *cart*, el cual es capaz de escoger clasificación o árboles de decisión en función de la naturaleza de la columna para predecir los NAs. Existen también otras técnicas más rápidas y sencillas como la media de los valores (*mean*) o asignar un valor aleatorio dentro del rango de valores de la columna (*sample*).

```{r}
cat("Dimensiones tras borrar columnas > 50% NAs y con sd=0\n")
dim(train_m2)
```

En primer lugar cabe destacar que este proceso es **muy costoso computacionalmente** por la gran cantidad de columnas que disponemos. Asimismo, cuanto más sofisticado sea el método a aplicar, más tiempo tardará en imputar los valores NA. Según esta fuente [12] a mayor número de imputaciones e iteraciones mejor resultado obtendremos al transformar los valores perdidos por los valores predichos. Sin embargo, tras realizar muchas pruebas he comprobado que para ciertas columnas **no existe suficiente información como para imputar todos los NAs** y que independientemente del número de iteraciones/imputaciones, el *dataset* resultante sigue conteniendo valores perdidos. Por lo tanto, a continuación se aplica la configuración que mejor relación calidad~tiempo he conseguido para imputar la mayoría de los valores NAs. Con ella pasamos de tener **988.826 valores perdidos a 11.003**, lo cual podemos determinar que es una gran reducción. Como anteriormente he comentado, este proceso es súmamente costoso y por ello solo se ha realizado una vez guardando en un fichero el *dataset* obtenido hasta el momento para sus posteriores usos.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Primera imputación de NAs utilizando clasificación y arboles de decisión ('cart')
train_m2_cart<-imputar_nas(train_m2, 1, 1, "cart")
# Almacenamos el dataset resultante en un fichero csv
write.csv(train_m2_cart,'train_m2_cart.csv', row.names = FALSE, col.names=TRUE)
```

Una vez disponemos de un primer conjunto con menos NAs, vamos a calcular la media para cada uno de los restantes de modo que terminemos de limpiar el *dataset* de valores perdidos. Lo he intentado realizar, de nuevo, con la función `mice` pero como he mencionado anteriormente, tiene dificultades para recopilar la información necesaria y calcular la media para asignarla en lugar del valor perdido. Por ello lo voy a realizar con un procedimiento particular consistente en calcular la media para cada una de las columnas que aún tengan valores perdidos reemplazándolos por la media obtenida. Cuando ya tenemos el conjunto libre de valores perdidos aplicamos la misma técnica anterior por la que eliminamos todas aquellas variables con un coeficiente de **correlación mayor que 0.75**. De este modo pasamos a tener **151 variables**.

En este segundo preprocesamiento, a diferencia del anterior, no vamos a estudiar la correlación entre `isFraud` y el resto de variables puesto que no vamos a eliminar ninguna que tenga una baja correlación. En su lugar aplicaremos el **algoritmo PCA** para reducir el número de variables. Sin embargo, previamente vamos a sustituir los *outliers* con la función que implementa la fórmula también aplicada en el anterior preprocesamiento.

```{r message=FALSE, warning=FALSE}
# Cargamos el dataset desde el fichero
train_m2_cart<-read.csv(file="./train_m2_cart.csv", header=TRUE, sep=",")
# Imputación de NAs
cat("Número de NAs tras la primera imputación con 'cart'\n")
sum(is.na(train_m2_cart))
cat("\nVariables con NAs\n")
cols_nas<-colnames(train_m2_cart)[!complete.cases(t(train_m2_cart))]
cols_nas
# Declaramos una función para calcular la media y asignársela a los valores NA de una columna
## El objetivo es poder aplicar el procedimiento a todo el dataset
asignar_media<-function(x) replace(x, is.na(x), mean(x, na.rm=TRUE))
train_m2_mean<-replace(train_m2_cart, TRUE, lapply(train_m2_cart, asignar_media))
cat("\nNúmero de NAs tras asignar la media\n")
sum(is.na(train_m2_mean))

# Correlación entre variables
train_m2_mean<-correlacion_variables(train_m2_mean, 0.75)
# Dimensión del conjunto resultante
cat("\nTras eliminar variables correladas > 0.75\n")
dim(train_m2_mean)
```

Tal y como podemos observar, de nuevo la fórmula para normalizar los *outliers* no termina de completar su objetivo, puesto que siguen quedando este tipo de valores tras aplicar la técnica. Sin embargo, en este segundo preprocesamiento también los dejaremos en el *dataset*. Una segunda consecuencia de haber tratado los *outliers* consiste en la generación de **nuevas columnas con desviación estándar igual a 0**. Este hecho ha sido descubierto al obtener un error aplicando el PCA referente a la imposibilidad de tratar variables con desviación estándar 0. Por lo que, de nuevo, volveremos a aplicar una reducción de las variables con esta cualidad. De este modo hemos pasado de disponer de **151 variables a 125**. Como todavía es un número considerable de variables, el siguiente paso es **aplicar el PCA** para realizar combinaciones de las anteriores variables y así reducir su dimensionalidad. 

```{r message=FALSE, warning=FALSE}
# Estudio de los outliers
cat("\nOutliers antes del tratamiento\n")
head(summary(estudio_outliers(train_m2_mean)), 10)
# Tratamiento de outliers mediante la fórmula
train_m2_out<-outliers_formula(train_m2_mean)
# Estudio del conjunto resultante
cat("\nOutliers tras el tratamiento\n")
head(summary(estudio_outliers(train_m2_out)), 10)

# Eliminamos las nuevas columnas con sd=0
train_m2_out<-eliminar_columnas_sd(train_m2_out, FALSE)
cat("\nDimensión tras eliminar de nuevo columnas con sd=0\n")
dim(train_m2_out)

# Aplicamos el PCA con un 80% de máxima varianza
# según el estudio realizado.
train_pca<-pca(train_m2_out, 0.8)
cat("\nDimensión tras aplicar el PCA con 80% de varianza\n")
dim(train_pca)
```

Si bien se puede especificar la máxima varianza que deben explicar las variables, para determinar el valor con mejor relación *calidad~número de variables* he entrenado modelos con valores 0.8, 0.85 y 0.9. Los resultados obtenidos se pueden visualizar en las siguientes tablas para Naive Bayes y Random Forest, respectivamente.

| Atributos | 0.7 | 0.8 | 0.9  |
|---|---|---|---|
| Nº variables  | 35  | 49  | 70  |
| Precisión  |  70% | 71%  | 71%  |
| Kappa  | 41%  | 42%  |  43% |

| Atributos | 0.7 | 0.8 | 0.9  |
|---|---|---|---|
| Nº variables  | 35  | 49  | 70  |
| Precisión  | 80% | 81%  | 81%  |
| Kappa  | 61% | 62%  |  63% |

Tal y como se puede observar, no existen grandes diferencias entre los tres umbrales de máxima varianza. Existe cierta mejoría del 70% al 80% tanto en *accuracy* como en el coeficiente de *Kappa* mientras que el aumento del número de variables no es demasiado significativo. Por el contrario, entre el 80%-90% sí podemos observar que el número de columnas se incrementa bastante en relación con la poca mejoría que ello conlleva. Por lo tanto para aplicar el algoritmo **PCA se establece un umbral del 80%**.

```{r}
# Transformamos la variable a predecir de numérica a factor para 
# que los modelos la detecten y realicen las predicciones
train_pca$isFraud<-as.factor(train_pca$isFraud)
# Dividimos el conjunto final en 70% entrenamiento y 30% test
datos<-get_train_test(train_pca, 0.7)
# Mostramos el resultado
cat("Conjunto de entrenamiento\n")
dim(datos$train)
cat("\nConjunto de test\n")
dim(datos$test)
cat("\n")

# MODELO NAIVE BAYES
set.seed(32)
library(e1071)
# Entrenamos el modelo utilizando todas las características
modelo_nb<-naiveBayes(isFraud~., data = datos$train)
# Obtenemos las predicciones y matriz de confusión
calidad_nb<-calidad_modelo(modelo_nb, datos$test, datos$test$isFraud)
confusionMatrix(calidad_nb$conf, positive='1')
```

El modelo entrenado con **Naive Bayes** ha obtenido una precisión del 70% bajo un coeficiente de Kappa del 41%. En comparación con los resultados del anterior modelo podemos observar que este ha empeorado ligeramente puesto que el valor de Kappa es bastante más bajo que el anterior. Acompañando a este fenómeno se encuentra la matriz de confusión en la que podemos visualizar que prácticamente se equivoca en un 30% indicando que una transacción es fraudulenta cuando no lo es, y algo más para el caso contrario. A continuación se aprecia la curva ROC de este clasificador.

```{r message=FALSE, warning=FALSE}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_nb$preds, datos$test$isFraud)
```

En este gráfico podemos observar que la capacidad de generalización de este clasificador es algo más baja que en la del primer preprocesamiento, aunque son bastante similares. Si resumimos los datos podemos determinar que pese a haber introducido tres cambios con respecto al preprocesamiento anterior (no estudiar la correlación entre las variables~isFraud, imputar NAs en lugar de borrarlos y utilizar el PCA para eliminar más columnas) este clasificador presenta un comportamiento muy parecido puesto que sigue fallando en aproximádamente un 30% de ocasiones. Tanto el valor de *Kappa* como la precisión han disminuido puesto que el volumen de datos de validación es mayor, y por lo tanto, con un grupo más grande de datos se pueden realizar un mayor número de tests y por ende se computa un mayor número de fallos.

```{r}
# MODELO RANDOM FOREST
set.seed(32)
library(randomForest)
# Entrenamos el modelo utilizando todas las características
modelo_rf<-randomForest(formula=isFraud~., data = datos$train, ntree=100)
# Obtenemos las predicciones y matriz de confusión
calidad_rf<-calidad_modelo(modelo_rf, datos$test, datos$test$isFraud)
confusionMatrix(calidad_rf$conf, positive='1')
```

En el caso del modelo entrenado con **Random Forest** podemos apreciar que los resultados también son peores con respecto al del primer preprocesamiento. En este caso se obtiene una precisión del 81% con un coeficiente de *Kappa* aproximadamente del 63%. De nuevo se puede observar una tendencia a la alza de errores al clasificador una transacción fraudulenta como una no fraudulenta, por lo que si en ambas técnicas sucede quiere decir que los datos no son lo suficientemente representativos como para terminar de diferenciar ambas clases. Sin embargo, este clasificador no ha empeorado tanto como el de Naive Bayes basándome en el valor de *Kappa* puesto que si bien no alcanza el valor óptimo del 80%, no ha bajado 20 puntos como ocurrió con el anterior. Esto nos indica que *Random Forest* es una técnica más robusta frente a la variación de los datos. Este hecho acompaña a la visualización de la **curva ROC** puesto que es muy parecida a la del primer preprocesamiento puesto que su área solo ha descendido dos puntos.

```{r message=FALSE, warning=FALSE}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_rf$preds, datos$test$isFraud)
```

### Conclusiones

Hasta el momento hemos realizado dos preprocesamientos diferentes. Las conclusiones que podemos determinar hasta el momento comparando los modelos entrenados con sendos son las siguientes:

* No es beneficioso submuestrear el conjunto al comienzo del preprocesamiento puesto que existen muy pocas transacciones positivas y al realizarlo de forma aleatoria podemos perder muchas de ellas, como ha surgido en la primera combinación.

* Aunque es una técnica mucho más costosa computacionalmente y puede presentar ciertas dificultades es mejor imputarlos valores perdidos que eliminar las filas que los contengan puesto que se pierden muchos registros y se reduce drásticamente el conjunto de datos con el que entrenar y validar.

* La fórmula utilizada para tratar los *outliers* tiene la desventaja de que estos son pasados por alto si no superan un determinado umbral. Si bien es un método muy sencillo para una primera aproximación, no creo que sea el más conveniente para utilizar.

* **Naive Bayes** es una técnica sencilla y rápida con la que entrenar un clasificador binario pero es muy sensible a la variación de los datos por lo que sus clasificadores no generalizan bien. Mientras que **Random Forest** es un poco más costosa computacionalmente pero mucho más robusta a las características cambiantes de los datos por lo que sus clasificadores tienen mejor capacidad de predicción y no han presentado diferencias significativas en ambos preprocesamientos.

## Modelos del tercer preprocesamiento

En

# Bibliografía

[1] Kaggle, IEEE-CIS Fraud Detection, https://www.kaggle.com/c/ieee-fraud-detection/data

[2] RDocumentation, df_status, https://www.rdocumentation.org/packages/funModeling/versions/1.9.3/topics/df_status

[3] Kaggle, Data Description (Details and Discussion), https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203

[4] Documentación sobre la función nearZeroVar, https://www.rdocumentation.org/packages/caret/versions/6.0-85/topics/nearZeroVar

[5] Documentación sobre la función mice, https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice

[6] Documentación acerca de la función findCorrelation, https://rdrr.io/rforge/caret/man/findCorrelation.html

[7] r-statistics.co, Selva Prabhakaran, Outlier Treatment, http://r-statistics.co/Outlier-Treatment-With-R.html

[8] Documentación sobre la función boxplot, https://www.rdocumentation.org/packages/grDevices/versions/3.6.2/topics/boxplot.stats

[9] Documentación sobre la función preProcess, https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/preProcess

[10] Documentación de la función confusionMatrix, https://www.rdocumentation.org/packages/caret/versions/3.45/topics/confusionMatrix

[11] DEGREES OF BELIEF, COMMON EVALUATION MEASURES FOR CLASSIFICATION MODELS, https://degreesofbelief.roryquinn.com/common-evaluation-measures-for-classification-models

[12] DATANOVIA, INTER-RATER RELIABILITY MEASURES IN R: Cohen’s Kappa in R: For Two Categorical Variables
, https://www.datanovia.com/en/lessons/cohens-kappa-in-r-for-two-categorical-variables/#interpretation-magnitude-of-the-agreement

[13] StackExchange, How do the number of imputations & the maximum iterations affect accuracy in multiple imputation?, https://stats.stackexchange.com/questions/219013/how-do-the-number-of-imputations-the-maximum-iterations-affect-accuracy-in-mul/219049

