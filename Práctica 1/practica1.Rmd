---
title: "Práctica 1. Pre-procesamiento de datos y clasificación binaria."
author: "Lidia Sánchez Mérida"
output: pdf_document
---

# Conjuntos de datos

En esta práctica se procede a analizar el conjunto de datos procedente de *Kaggle* y denominado **IEE-CIS Fraud Detection** [1]. En él se distinguen hasta dos tipos de documentos. Aquellos que contienen el término `identity` contienen los datos personales de los individuos que han realizado transacciones bancarias. Mientras que los ficheros que contienen la palabra `transaction` disponen de la información asociada a cada una de las transacciones realizadas. Ambos ficheros son relacionables a través de un campo denominado `TransactionID`, el cual nos permite conocer más detalles acerca de las personas que han realizado las transferencias. Sin embargo, de muchas de ellas no se conoce esta información.

Para cada uno de estos tipos de ficheros existe su correspondiente conjunto de entrenamiento y validación ya separados, pero como todos los conjuntos de datos son tan amplios voy a usar directamente los ficheros `train_innerjoin.csv` y `test_innerjoin.csv` proporcionados en el repositorio de GitHub de la asignatura.

```{r message=FALSE, warning=FALSE}
# Establecemos una semilla para que los resultados sean reproducibles.
set.seed(32)
# Leemos los datos desde los ficheros
train<-read.csv(file="./train_innerjoin.csv", header=TRUE, sep=",")
test<-read.csv(file="./test_innerjoin.csv", header=TRUE, sep=",")
# Dimensiones de los conjuntos
dim(train)
dim(test)
```

Tras cargar los datos podemos observar, como anteriormente he puntualizado, que ambos cuentan con un número considerablemente amplio tanto de registros como de variables. Para conocer más información acerca de los conjuntos de datos vamos a realizar un análisis exploratorio que nos permita identificar los principales aspectos más relevantes.

# Análisis exploratorio

## Estado de los datos

Primeramente vamos a conocer el estado de los datos. Mediante la función `df_status` [2] podremos conocer los valores de todos los campos de un conjunto con el objetivo de conocer la cantidad y el porcentaje de ceros, valores nulos o infinitos. Asimismo, en la última columna también nos indica la cantidad de valores únicos que existen para cada campo. De este modo podemos conocer, por ejemplo, si una variable es categórica. Para conocer cuáles podrían ser los campos de mayor relevancia, podemos leer desde el propio Kaggle qué es lo que representa cada uno de ellos [3].

```{r message=FALSE, warning=FALSE, include=FALSE}
# Obtenemos el estado de los dos conjuntos de transacciones
library(funModeling)
train_st<-df_status(train)
test_st<-df_status(test)
```

Si bien el resultado proporcionado por estas funciones es súmamente extenso como para imprimirlo, he realizado un análisis acerca de las variables más relevantes obteniendo las siguientes conclusiones:

* En primer lugar destacamos que la variable categórica `isFraud` que intentamos predecir dispone de un altísimo número de ejemplos de transacciones no fraudulentas en el conjunto de entrenamiento. Tal es así que apenas existe un 8% de transacciones clasificadas como fraudulentas. Si bien este tipo de fenómenos es bastante común, clases tan súmamente desbalanceadas suelen dificultar el proceso de entrenamiento y obtención de buenos clasificadores.
* Casi la mitad de las variables disponen de aproximádamente un 50% de valores perdidos. Mayoritariamente se encuentran relacionadas con medidas tales como la distancia, características como la dirección, entre otras.
* Existen bastantes variables categóricas, además de `isFraud` que indican los dominios origen y destino, el tipo de dispositivo desde el que se hizo la transacción, el producto de la misma, entre otras.

## Relación dinero-fraude

A continuación procedo a representar gráficamente otro tipo de estadísticas que pueden también ser interesantes para conocer, un poco, las características del tipo de transacciones. En primer lugar vamos a averiguar si existe algún tipo de **relación entre la cantidad de la transacción y su clasificación como fraudulenta o no**. El objetivo es conocer si existe un patrón para detectar las transacciones fraudulentas, como por ejemplo si en muchas de ellas se han traspasado grandes cantidades de dinero. Para ello vamos a dibujar un histograma para representar la variable `TransactionAmt`, que es la que contiene las sumas de dinero, y las diferenciaremos en función de si son o no fraudulentas. Para este último paso deberemos de transformar los valores de la columna `isFraud` a categóricos  

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
# Transformamos 0->No, 1->Yes para diferenciar las transacciones
# fraudulentas de las que no lo son.
train_categ<-train %>% mutate(isFraud = as.factor(ifelse(isFraud == 1, 'Yes', 'No')))
# Representamos las cantidades y si son de transacciones fraudulentas o no.
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionAmt),
     fill = as.factor(isFraud)), binwidth = 100)
```

Como se puede observar no existen grandes diferencias entre las cantidades monetarias de las transacciones clasificadas como fraudulentas de las que no lo son. Sin embargo, este histograma nos permite comprobar que la **mayoría de transacciones se encuentran agrupadas** dentro de un rango aproximado [0-700], lo cual nos posibilita realizar un **corte horizontal para reducir el número de registros** a los contenidos en dicho rango.

## Relación tiempo-fraude

Aprovechando los dos conjuntos separados de transacciones, vamos a estudiar como segundo caso la **relación entre el tipo de transacción y la medida de tiempo asociada** en el campo `TransactionDT`. Esta representa la diferencia entre dos valores temporales, y aunque si bien en la descripción del dataset no especifican las medidas utilizadas, sus valores se basan en los de los campos DX donde X es un número entre 1 y 15. De estos conocemos por la descripción en Kaggle que se consideran, por ejemplos, los días transcurridos entre una transacción y otra. El objetivo de este análisis consiste en averiguar si el rango temporal entre dos transacciones puede ser un factor a considerar para averiguar si son o no fraudulentas.

```{r}
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionDT),
    fill = as.factor(isFraud)), binwidth = 1000000)
```

Tal y como se puede observar, aquellas transacciones marcadas como **fraudulentas disponen de un valor menor que las no fraudulentas**, por lo que podemos considerar que quizás este factor de tiempo puede estar relacionado con la naturaleza legal de la transacción. Una posible teoría que explique este suceso puede basarse en que cuando, por ejemplo hacemos algo que está mal, intentamos que sea lo más rápidamente posible. En análisis posteriores comprobaremos si efectívamente existe algún tipo de relación entre esta variable y la columna a predecir que nos ayude a entrenar modelos predictivos.

# Preprocesamiento de datos

En esta sección procedemos a aplicar diversas técnicas para mejorar la calidad del conjunto de datos de modo que podamos obtener un subconjunto más sencillo con el que entrenar los futuros modelos predictores. Uno de los principales problemas de este conjunto reside en su **gran dimensionalidad** tanto del número de registros como de columnas. El hecho de disponer de un enorme grupo de muestras puede provocar problemas de rendimiento a la hora de intentar entrenar un modelo con tal volumen de datos. Mientras que disponer de tal cantidad de variables también puede producir grandes tiempos de espera en entrenamiento al considerar tantas columnas para obtener el clasificador. Estos son los dos principales problemas que se abordan a continuación.

## Reducción del número de muestras aleatoriamente

En primer lugar debo reducir el número de registros del conjunto de datos para poder trabajar con él. Para ello, en primer lugar, vamos a realizar el corte horizontal hablado anteriormente y referente al **análisis exploratorio de la cantidades monetarias** de las transacciones. De este modo obtendremos un conjunto de datos más reducido y orientado hacia el grupo mayoritario de transacciones. Para ello vamos a eliminar aquellos registros con un cantidad superior a 700 USD.

```{r}
library(dplyr)
library(magrittr)
minitrain_700<-train %>% filter(TransactionAmt < 700)
cat("Dimensiones al quitar transacciones > 700 USD\n")
dim(minitrain_700)
# Escogemos 50.000 muestras aleatoriamente del conjunto de entrenamiento
set.seed(32)
minitrain<-minitrain_700[sample(nrow(minitrain_700), 50000), ]
```

Tal y como podemos comprobar, hemos conseguido reducir casi 1000 registros. Sin embargo, este conjunto de datos sigue siendo inmanejable por lo que es necesario aplicar una reducción aún mayor. Para ello vamos a realizar un muestreo aleatorio utilizando la función `sample` [4]. El objetivo es seleccionar 50.000 individuos de forma aleatoria incluyendo todas las columnas del dataset. De este modo podremos obtener un conjunto de datos más sencillo con el que comenzar a trabajar.

## Variables con demasiados valores perdidos

Comenzamos abordando otro de los principales problemas de este conjunto de datos que consiste en la aparición numerosa de valores perdidos o NAs. Si bien uno de los métodos posibles para eliminarlos es predecirlos utilizando alguna técnica que nos permita obtener un valor aproximado a partir de la información que existe en el conjunto, si el número de estos valores es sumamente predominante **(> 50%)** es prácticamente imposible aplicar este método. Por lo tanto, a continuación se desarrolla una función en la que se eliminarán aquellas columnas del dataset que dispongan de un número de valores perdidos superior al 50% de los datos totales. Para ello me he basado en los ejemplos de los *scripts* proporcionados en la asignatura.

```{r message=FALSE, warning=FALSE, include=FALSE}
eliminar_columnas_nas<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(funModeling)
  estado<-df_status(datos)
  # Obtenemos aquellas columnas con más de un 50% de valores perdidos (NAs) a partir del estado
  na_cols <- estado %>%
    filter(p_na > 50) %>%
    select(variable)
  # Eliminamos las columnas obtenidas
  remove_cols <- bind_rows(
    list(na_cols)
  )
  # Obtenemos el conjunto de datos reducido 
  datos_finales <- datos %>%
    select(-one_of(remove_cols$variable))
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Variables irrelevantes

Analizando más en profundidad la información que nos aportan las columnas desde la propia página del dataset [1] me he percatado de que las variables **C1-C14 están cifradas**, lo cual quiere decir que si no disponemos de la clave de encriptado no podemos acceder a la información real, y por tanto, disponemos de 14 columnas con información que no significa nada. Por ello, como segundo paso de este preprocesamiento se desarrolla la siguiente función que se encarga de eliminar dichas columnas dado un dataset.

```{r}
eliminar_columnas_encriptadas<-function(datos) {
  # Eliminamos las columas C1-C14
  datos_finales<-datos[, -which(colnames(datos) %in% c("C1", "C2", "C3", "C4", 
   "C5", "C6", "C7", "C8", "C9", "C10", "C11", "C12", "C13", "C14"))]
  # Devolvemos el conjunto resultante
  datos_finales
}
```

También se consideran variables irrelevantes aquellas que no aportan información útil debido a que sus **valores son muy similares**. Este tipo de columnas no nos ayudan a predecir, posteriormente, si una transacción es o no fraudulenta puesto que independientemente de las muestras su comportamiento no varía. Uno de los métodos por los que se puede decidir si una variable dispone de valores interesantes es mediante la **desviación estándar**. Si esta medida estadística es 0 entonces nos indica que no existe variabilidad en los valores de la columna, y por tanto, esta variable no será útil. 

Para realizar este preprocesamiento vamos a hacer uso de la función `nearZeroVar` [5] que calcula el número de valores únicos y su frecuencia de aparición. De este modo obtendremos aquellas variables cuya desviación estándar sea 0 o cercana a dicho valor. Previo a su aplicación, es necesario transformar todas las variables a numéricas para poder calcular la desviación estándar. Para las columnas categóricas se asignará a cada etiqueta un único valor que la represente.

```{r}
eliminar_columnas_sd<-function(datos, eliminar_todas) {
  # Convertimos todas las variables a variables numéricas
  for(i in c(1:ncol(datos))) {
       datos[,i] <- as.numeric(datos[,i])
  }
  # Calculamos el número de variables con desviación estándar cercana o igual a 0.
  library(caret)
  nz<-nearZeroVar(datos, names = TRUE, saveMetrics = TRUE)
  # Desviación estándar = 0
  nz0<-nz[nz[,"zeroVar"] > 0, ]
  # Desviación estándar cercana a 0
  nz_cercano0<-nz[nz[,"zeroVar"] + nz[,"nzv"] > 0, ]
  # Eliminamos las variables con desviación estándar = 0
  datos_finales<-datos[, -which(colnames(datos) %in% rownames(nz0))]
  
  # Si eliminar_todas=TRUE entonces también eliminamos las variables con sd cercana a 0
  if (eliminar_todas == TRUE) {
    datos_finales<-datos_finales[, -which(colnames(datos_finales) %in% rownames(nz_cercano0))]
  }
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Tratamiento de valores perdidos

En este apartado procedemos a tratar los numerosos valores perdidos que contiene el conjunto de datos. Si bien existen multitud de técnicas para aplicarle a este tipo de datos, yo voy a considerar las siguientes:

* **Eliminación de NAs**. Esta puede ser la técnica más drástica para tratar los valores perdidos. Para llevarla a cabo vamos a aplicar la función `na.omit()` que eliminará todos los registros que contentan este tipo de valores.
* **Imputación de NAs**. Al contrario que la anterior, esta técnica más sofisticada es una de las más utilizadas según he estado investigando. Para aplicarla existen varias librerías pero al parecer la más popular es `mice` debido a las múltiples operaciones que puede llevar a cabo para asignar un valor a cada NA [6]. Dependiendo de la naturaleza de los datos, existen métodos específicos aunque también dispone de métodos generales aplicables a cualquier tipo de dato.

A continuación se expone el código de las funciones asociadas a las dos técnicas explicadas anteriormente.

```{r}
eliminar_nas<-function(datos) {
  datos_finales<-na.omit(datos)
  # Devolvemos el conjunto resultante
  datos_finales
}

imputar_nas<-function(datos, imputs, iters, metodo) {
  library(mice)
  # Especificamos el número de imputaciones, el número de iteraciones
  # por imputación y el método a utilizar para imputar los NAs
  modelo_mice<-mice(minitraint_sd, m=imputs, maxit=iters, method=metodo)
  # Obtenemos el conjunto de datos imputado
  datos_imputados<-complete(modelo_mice)
  # Comprobamos el número de NAs que han quedado
  anyNA(datos_imputados)
  # Devolvemos el conjunto resultante
  datos_imputados
}
```

## Correlación

En esta sección vamos a estudiar la **correlación existente entre las variables y la columna a predecir**, que en el caso de este *dataset* es `isFraud`, y posteriormente analizaremos cuán **relacionadas se encuentran las variables entre sí**. El objetivo es, por un lado, conocer si existe alguna variable o grupo de variables que expliquen el comportamiento de la variable categórica a predecir. Mientras que por otro lado queremos conocer si existen variables muy correladas entre sí de manera que podamos reducir la dimensionalidad eliminando aquellas cuyo coeficiente de correlación sea muy alto, ya que este sería un indicio de que dichas columnas aportan la misma información.

Para el primer estudio vamos a aplicar la función `correlation_table` tal y como se ejemplifica en uno de los *scripts* proporcionados en la asignatura. Como precondición para calcular los coeficientes de correlación, es necesario que las columnas dispongan solo de valores numéricos. 
De igual forma, esta condición es necesaria para el segundo análisis que involucra a todas las variables, para el cual usaremos, en primer lugar, la función `cor` que se encargará de calcular los coeficieintes de correlación entre todas las variables del conjunto. Una vez dispongamos de esta matriz, aplicaremos la función `findCorrelation` [7] que nos ayudará a identificar aquellas variables que tienen un coeficiente de correlación especificado como argumento. De este modo se puede personalizar el nivel de exigencia con el que eliminar las columnas correladas.

A continuación se presentan las funciones asociadas a los dos estudios de correlación que se realizarán a posteriori.

```{r message=FALSE, warning=FALSE}
correlacion_fraude<-function(datos) {
  library(funModeling)
  # Calculamos los coeficientes de correlación en torno a la variable clasificatoria
  tabla_corr<-correlation_table(datos, target='isFraud')
  tabla_corr
}

correlacion_variables<-function(datos, max_corr) {
  library(caret)
  # Calculamos los coeficientes de correlación todas-todas las variables.
  coefs<-cor(datos)
  # Excluimos las filas con valores perdidos porque si no esta función no se puede aplicar.
  cols_corr<-findCorrelation(na.omit(coefs), cutoff=max_corr)
  # Eliminamos las columnas resultantes del dataset
  datos_finales<-datos[-c(cols_corr)]
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Outliers

Los *outliers* son datos cuyos valores están caracterizados por encontrarse fuera del rango normalizado para el atributo. En otras palabras, este tipo de datos disponen de valores muy diferentes a los del resto del conjunto y por ende pueden afectar a la capacidad de generalización de los modelos entrenados. Por este motivo, esta sección se va a dedicar al estudio y tratamiento de este tipo de valores. Para el análisis me he inspirado en este ejemplo [8] en el que se hace uso de la función `boxplot` [9], con la que se calculan los valores situados fuera del rango particular de cada variable. De este modo podremos conocer los *outliers* de cada una de las columnas.

```{r message=FALSE, warning=FALSE}
estudio_outliers<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Calculamos los outliers de todas las variables del conjunto de datos.
  outliers<-datos %>%
        map(~ boxplot.stats(.x)$out) 
  # Devolvemos los resultados
  outliers
}
```

Una vez disponemos de información acerca de los *outliers* existentes en el conjunto de datos, procedemos a estudiar los diferentes tratamientos que se les pueden aplicar. Dentro del ejemplo mecionado anteriormente se presentan varias técnicas, de las cuales he escogido las dos más interesantes a mi parecer: **la imputación mediante una determinada fórmula y la predicción de los valores**. La primera técnica consiste en reemplazar aquellos valores que se sitúen fuera del rango de los cuantiles 25 y 75, por el cuantil 5 o el 95, respectivamente. El objetivo es transformar los *outliers* que se encuentran por debajo del rango en el valor más pequeño que se encuentra dentro de dicho intervalo, y realizar el mismo procedimiento con los que se encuentran por encima del rango pero, en este caso, asignando el mayor valor posible.

La segunda técnica consiste en convertir todos los *outliers* detectados en valores NA para posteriormente imputarlos. Como ya disponemos de una función que se encarga de tratar los valores perdidos, será esa que reutilizaremos para predecir este tipo de valores y así probar esta segunda técnica. A continuación se presentan las funciones asociadas a los dos tratamientos explicados anteriormente.

```{r message=FALSE, warning=FALSE, include=FALSE}
outliers_formula<-function(datos) {
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_finales<-datos[, -which(names(datos) %in% c("isFraud"))]
  i <- 1
  for(col in datos_finales) {
    # Calculamos los cuantiles 25 y 75 para comprobar es un outlier
    quantiles<-quantile(col, probs=c(0.25, 0.75))
    # Calculamos los cuantiles 5 y 95 para asociar como nuevos valores a los outliers
    nuevos_valores<-quantile(col, probs=c(0.05, 0.95))
    # Calculamos la varianza máxima que puede sufrir un valor
    H<-1.5*IQR(col)
    # Sustitución de outliers
    col[col < (quantiles[1] - H)] <- nuevos_valores[1]
    col[col > (quantiles[2] + H)] <- nuevos_valores[2]
    # Actualizamos el dataset
    datos_finales[i]<-col
    i <- i + 1
  }
  # Volvemos a añadir la columna `isFraud`
  datos_finales<-cbind(datos_finales, isFraud=datos$isFraud)
  # Devolvemos los datos resultantes
  datos_finales
}

outliers_prediccion<-function(datos, imputs, iters, metodo) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_sin_fraud<-datos[, -which(names(datos) %in% c("isFraud"))]
  # Transformamos los valores outliers en valores perdidos (NAs)
  datos_nas<-as.data.frame(apply(datos_sin_fraud, 2, function(row){row[row %in% boxplot(row, plot = FALSE)$out] = NA; row})) 
  datos_finales<-imputar_nas(datos_nas, imputs, iters, metodo)
  # Devolvemos el dataset resultante
  datos_finales
}
```

## Selección de variables

El objetivo de este apartado consiste en reducir la dimensionalidad del *dataset* en lo referente a las variables que nos permiten predecir la columna categórica `isFraud`. Si bien existen diferentes técnicas para llevar este procedimiento a cabo, vamos a utilizar una de las más populares denominada **Análisis de Componentes Principales (PCA)**. Su procedimiento consiste en realizar diversas combinaciones lineales de los atributos del dataset de modo que nos indique cuáles son los más relevantes a considerar. Este tipo de técnicas se deben aplicar solo cuando el número de variables es considerablemente amplio, puesto que si lo llevamos a cabo con un número de columnas razonable podemos perder información al eliminar algunas de ellas. 

Si bien existen dos paquetes principales para aplicar el PCA con y sin valores perdidos, vamos a hacer uso de este último puesto que en esta sección se presupone que el conjunto de datos se encuentra preprocesado e imputado. Asimismo, el PCA se puede ver altamente influido por aquellas variables con una mayor varianza lo que supondría su predominio sobre el resto de columnas. Es por ello por lo que el planteamiento de esta técnica va a ser similar al de este ejemplo [12]. En él se utiliza la función `prcomp()` [13] que previo a la aplicación del algoritmo PCA, es capaz de analizar las variables y aplicar procedimientos de **centrado y escalado** para que su media sea 0 y su desviación estándar 1.

**HAY GRÁFICOS EN LOS EJEMPLOS BUSCADOS Y EN EL TRABAJO3 - Al aplicar finalmente el algoritmo PCA (que lo haremos directament con \textbf{preProcess()}), obtendremos un conjunto reducido de "atributos". Lo expresamos entre comillas, ya que en realidad, estos valores son combinaciones lineales de los atributos iniciales, por tanto, como resultado habremos conseguido reducir en dimensionalidad a costa de tener datos mucho menos interpretables.**
**APLICAREMOS LUEGO PREPROCESS Y PREDICT PARA OBTENER EL CONJUNTO DE DATOS REDUCIDO?????????**


## Balanceo de la clase a predecir

En esta última sección se tratará el desbalanceo de la clase `isFraud` con la que debemos entrenar los modelos para poder predecir si una transacción es fraudulenta o no. Como hemos podido observar en el análisis exploratorio, la gran mayoría de muestras son transacciones no fraudulentas (cerca del 96%) mientras que la clase minoritaria es la asociada a las transacciones fraudulentas. Este tipo de problemas es bastante común, pero en el caso de este dataset **es súmamente desproporcionado**. Para intentar paliar este inconveniente se van a aplicar dos de las técnicas más populares: **downsampling y (?)**. En el primer caso se obtendrá un subconjunto de datos en el que se iguale el número de muestras de ambas clases, es decir, se escogerán tantas muestras de transacciones no fraudulentas al azar como ejemplos de transacciones fradulentas haya [downsample]. Con esta técnica el conjunto de datos se reducirá drásticamente, lo cual provocará una serie de consecuencias que analizaremos posteriormente al entrenar los modelos.

```{r}
downsampling<-function(datos) {
  library(caret)
  # Obtenemos las etiquetas a predecir
  etiquetas<-factor(datos$isFraud)
  # Downsampling para igualar ambas clases
  set.seed(32)
  datos_finales<-downSample(datos[, -which(names(datos) %in% c("isFraud"))],
      etiquetas, yname='isFraud')
  # Comprobamos que ambas clases se encuentran balanceadas
  count(datos_finales, isFraud)
  # Devolvemos los datos resultantes
  datos_finales
}
```

## División del conjunto de datos

Por último vamos a introducir una función cuyo objetivo consiste en dividir un conjunto de datos en **entrenamiento y validación**. La razón de ser reside en la posibilidad de poder calcular un mayor número de medidas de calidad tras validar los modelos con el conjunto de test más que considerar solamente la tasa de error que nos devolvería Kaggle si subimos los datos. Para ello vamos a hacer uso de la función `createDataPartition`[] a la cual se le proporciona el conjunto de datos a dividir especificando la variable a predecir, el número de muestras para el conjunto de entrenamiento y el número de particiones.

```{r}
get_train_test<-function(datos, porcentaje_train) {
  # Mezclamos los datos aleatoriamente
  set.seed(32)
  datos<-datos[sample(1:nrow(datos)), ]
  # Dividimos el conjunto en train y test
  library(caret)
  train<-createDataPartition(datos$isFraud, p=porcentaje_train, list=FALSE, times=1)
  datos_train<-datos[train, ]
  datos_test<-datos[-train, ]
  # Devolvemos ambos dataframes en una lista
  df<-list()
  df$train<-datos_train
  df$test<-datos_test
  df
}
```

# Modelos

Esta sección se encuentra orientada a producir diversos modelos predictivos utilizando varias combinaciones de las técnicas de preprocesamiento explicadas anteriormente. El objetivo es estudiar si la capacidad de generalización de los modelos mejora o empeora en función de los diferentes cambios de preprocesamiento que introduzcamos. Para todas las combinaciones vamos a aplicar los mismos métodos de entrenamiento para que los modelos obtenidos sean comparables y así basar el análisis en función de las técnicas de preprocesamiento de datos. Para ello voy a utilizar, principalmente, la librería `e1071` con el objetivo de entrenar modelos utilizando la técnica **Naive Bayes**, la cual se caracteriza por su rapidez y por estar orientada a la clasificación binaria. Como segunda técnica utilizaré **Random Forest** por ser más robusta y sofisticada que la anterior con el fin de intentar obtener un modelo más refinado que sea capaz de proporcionar mejores resultados. Para ello haré uso de la librería de igual nombre `randomForest`.

A continuación se presentan dos funciones con las que calcular las predicciones realizadas dado un modelo así como su matriz de confusión y toda la información asociada a la calidad del mismo. Por otro lado también se incluye una segunda función para dibujar la curva ROC y calcular el área debajo de esta para poder comprobar visualmente la capacidad de generalización de un modelo proporcionado. Se proponen como funciones para poder utilizarlas en todos los modelos que se entrenen en esta sección.

```{r}
calidad_modelo<-function(modelo, test, etiquetas_test) {
  # Obtenemos las predicciones del modelo para el conjunto de test
  predicciones<-predict(modelo, test)
  # Obtenemos la matriz de confusión y la información 
  confusion<-table(etiquetas_test, predicciones, dnn = c("Real", "Predicha"))
  resultado<-list()
  resultado$preds<-predicciones
  resultado$conf<-confusion
  resultado
}

curva_roc<-function(predicciones, etiquetas) {
  library("ROCR")
  # Calculamos el porcentaje de acierto entre las etiquetas predichas y las reales
  preds<-prediction(as.numeric(predicciones), as.numeric(etiquetas))
  # Comprobamos la eficacia del modelo considerando los falsos positivos y los aciertos
  curva<-performance(preds, "tpr", "fpr")
  # Dibujamos la curva.
  plot(curva, col="green", add=FALSE, main="Curva ROC", lwd = 2) 
  segments(0, 0, 1, 1, col='black')
  grid() 
  # Calculamos el área debajo de la curva
  curva.area = performance(preds, "auc")
  cat("\nEl área bajo la curva ROC es", curva.area@y.values[[1]]*100,"%\n")
}
```

## Modelos del primer preprocesamiento

En este primer modelo vamos a comenzar eliminando aquellas columnas con más de un **50% de valores perdidos**. De este modo conseguimos reducir el número de variables de **434 a 305**, que si bien siguen siendo muchas solo eliminando las columnas con más de un 50% de NAs hemos conseguido una buena reducción del número de columnas. A continuación, procedemos a eliminar las **columnas consideradas irrelevantes** para continuar reduciendo la dimensionalidad del conjunto de modo que eliminando las columnas encriptadas y las que tienen desviación estándar 0 se consigue disponer de **289 columnas**.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Eliminamos columnas con > 50% NA
minitrain_m1<-eliminar_columnas_nas(minitrain)
# Eliminamos columnas irrelevantes: C1-C14 encriptadas y con sd=0
minitrain_m1<-eliminar_columnas_encriptadas(minitrain_m1)
minitrain_m1<-eliminar_columnas_sd(minitrain_m1, FALSE)
```

Una vez hemos reducido casi la mitad de las columnas del *dataset*, previo a realizar estudios más sofisticados vamos a **tratar los valores perdidos**. Para ello vamos a aplicar la primera técnica explicada anteriormente consistente en **eliminar todos los registros con NA**. De este modo podemos comprobar cómo el conjunto de datos se ve grávemente afectado puesto que pasa de 50.000 muestras a disponer solo de aproximadamente **7.509 ejemplos**. Como ya anticipamos este método es muy agresivo pero cuando entrenemos los modelos comprobaremos si este hecho se ve reflejado en su calidad.

Aplicando algunas técnicas para conocer la importancia de las 289 variables que disponemos me he dado cuenta de que al eliminar los valores perdidos algunas variables **vuelven a tener una desviación estándar igual a 0**. Por lo tanto vamos a realizar una segunda pasada de esta función para eliminarlas con el objetivo de poder, a continuación, estudiar la correlación entre variables y con la variable a predecir para continuar reduciendo la dimensionalidad asociada a las columnas. Tal y como comprobar han sido un total de tres variables las que han sufrido la desviación estándar 0 al eliminar los valores NA, por lo que deducimos que estas columnas no han sido eliminadas anteriormente por contar con este tipo de valores que les sumaban cierta variabilidad a su rango. Por lo tanto en este momento contamos con **284 variables**.

```{r}
# Eliminar registros con valores NA
minitrain_m1<-eliminar_nas(minitrain_m1)
# Volvemos a eliminar las nuevas columnas que han surgido con desviación estándar=0
# tras quitar valores perdidos
minitrain_m1<-eliminar_columnas_sd(minitrain_m1, FALSE)
# Dimensión del conjunto resultante
cat("\nSegunda tanda para eliminar variables con sd=0\n")
dim(minitrain_m1)

# Correlación entre variables
minitrain_m1<-correlacion_variables(minitrain_m1, 0.75)
# Dimensión del conjunto resultante
cat("\nTras eliminar variables correladas > 0.75\n")
dim(minitrain_m1)

# Correlación entre las variables y la categórica `isFraud`
tabla_corr<-correlacion_fraude(minitrain_m1)
head(tabla_corr, 10)
# Obtenemos las variables que tienen un coeficiente de correlación muy bajo con respecto a isFraud
tabla_corr_baja<-tabla_corr %>% filter(isFraud > -0.1 & isFraud < 0.1)
# Las eliminamos del conjunto de datos haciendo una copia en otro para no perder el original
minitrain_m1_corr<-minitrain_m1[, -which(colnames(minitrain_m1) %in% tabla_corr_baja$Variable)]
# Dimensión del conjunto resultante
cat("\nTras eliminar variables con correlación en [-0.1, 0.1] con respecto a isFraud\n")
dim(minitrain_m1_corr)
```

A continuación vamos a estudiar la **correlación entre variables**. Como se ha explicado al presentar su correspondiente función, nuestro objetivo es eliminar aquellas que proporcionen información redundante para así reducir más el número de columnas del *dataset*. Para ello vamos a establecer un umbral de **0.75** por el cual eliminaremos todas aquellas variables cuyos coeficientes sean mayores a dicho valor puesto que este conlleva un indicio de alta correlación entre columnas. Este procesamiento nos ha hecho bajar de 286 variables a solo **82 columnas**, por lo que podemos concluir que la gran mayoría de variables restantes proporcionaban la misma información y ahora vamos a considerar aquellas que son más relevantes.

En relación al análisis de **correlación entre las variables y la categórica `isFraud`** podemos visualizar los diez primeros resultados puesto que la tabla es considerablemente amplia como para mostrarla entera. Sin embargo, estudiándola en profundidad he podido percatarme de que existen muchas columnas con menos de 0.1 de correlación. Si bien en este caso el objetivo es el mismo de reducir el número de columnas del que disponemos, en este caso queremos disponer de aquellas **variables que tengan un mayor coeficiente de correlación con `isFraud`** puesto que este hecho nos indica que dichas columnas pueden explicar el comportamiento de la variable a predecir. Mientras que si bien las columnas con una baja correlación por sí solas no ayudan a predecir la variable categórica, sí que pueden aportan información valiosa si se combinan con otras columnas. Para comprobar esta teoría vamos a entrenar unos primeros modelos **eliminando aquellas variables con una correlación en el intervalo [-0.1, 0.1]** de modo que excluyamos todas aquellas columnas con una relación muy baja con la variable `isFraud` en base al coeficiente de correlación. Con este procesamiento ya disponemos de **35 variables**, un número mucho más manejable de columnas del que teníamos anteriormente.

A continuación vamos a realizar un estudio de los **outliers** para comprobar cuántos de estos valores disponen las 34 columnas del conjunto de datos resultante hasta el momento. De nuevo mostramos solo los 10 primeros resultados porque aun siendo menos columnas, la lista de *outliers* es bastante extensa para mostrarla al completo. Visualizándola en profunidad he podido observar que la gran mayoría de columnas disponen de este tipo de valores en mayor o menor cantidad, por ello se le va a aplicar el primero de los tratamientos explicados, consistente en transformarlos en el cuantil 5 o 95 en función de si se encuentran por debajo o por encima del rango de valores, respectivamente.

```{r message=FALSE, warning=FALSE}
# Estudio de los outliers
cat("\nOutliers antes del tratamiento\n")
head(summary(estudio_outliers(minitrain_m1_corr)), 10)
# Tratamiento de outliers mediante la fórmula
minitrain_m1_out<-outliers_formula(minitrain_m1_corr)
# Estudio del conjunto resultante
cat("\nOutliers tras el tratamiento\n")
head(summary(estudio_outliers(minitrain_m1_out)), 10)
```

Una vez hemos tratado los *outliers* de todas las columnas excepto `isFraud` volvemos a realizar un análisis para comprobar si aún existen este tipo de valores. Y tal y como podemos observar así es, algunas variables siguen disponiendo de este tipo de valores. La razón de ello reside en que si un valor *outlier* no supera el umbral que se calcula para la variable, no se modifica y por lo tanto pasa al nuevo conjunto resultante. Por el momento vamos a mantener este conjunto de datos para posteriormente comparar los modelos entrenados con él y con otro conjunto al que le apliquemos la segunda técnica explicada para los *outliers*.

Como ya disponemos de un conjunto de variables más reducido, **no vamos a aplicar el PCA** puesto que nos arriesgamos a perder información a la hora de combinar las columnas. Por lo que procedemos directamente a realizar el último paso previo al entrenamiento de los modelos: **balancear la clase a predecir**. Para ello vamos a aplicar la primera técnica explicada denominada **downsammpling**, con el objetivo de disponer del mismo número de transacciones fraudulentas como de no fraudulentas. Tal y como podemos observar el conjunto se ha reducido considerablemente puesto que el número de muestras fraudulentas es súmamente menor que el de no fraudulentas, y si a esto le sumamos partir de un conjunto con no demasiadas muestras pues al final el *dataset* dispone de pocos ejemplos.

Una vez disponemos del conjunto de datos preprocesado, lo dividimos en un conjunto de entrenamiento y test para validar los modelos que vamos a entrenar a continuación con el objetivo de calcular un mayor número de medidas que la tasa de error que solo nos devuelve Kaggle. Para ello vamos a separar el **70% de los datos para entrenamiento y el 30% dedicado a la validación de los modelos**.

```{r message=FALSE, warning=FALSE}
# Downsampling para balancear la clase `isFraud`
minitrain_m1_final<-downsampling(minitrain_m1_out)
cat("Conjunto de datos tras downsampling\n")
dim(minitrain_m1_final)
# Dividimos el conjunto final en 70% entrenamiento y 30% test
datos<-get_train_test(minitrain_m1_final, 0.7)
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nConjunto de test\n")
dim(datos$test)
cat("\n")

# MODELO NAIVE BAYES
set.seed(32)
library(e1071)
# Entrenamos el modelo utilizando todas las características
modelo_nb<-naiveBayes(isFraud~., data = datos$train)
# Obtenemos las predicciones y matriz de confusión
calidad_nb<-calidad_modelo(modelo_nb, datos$test, datos$test$isFraud)
confusionMatrix(calidad_nb$conf, positive='1')
```

Como podemos observar hemos utilizado la función `confusionMatrix` [10] para obtener más información acerca del modelo a partir de la propia matriz de confusión. Si bien la documentación oficial de la función no aporta información acerca de los atributos mostrados, he encontrado un ejemplo [11] en el que se detallan cada uno de ellos. En primer lugar destacamos que la **precisión del modelo es de un 77%**, valor que se encuentra dentro de un intervalo al 95%, lo que nos asegura casi con totalidad que la precisión del modelo mostrada es verídica. Por otro lado podemos observar en el atributo *No Information Rate* cómo la función ha detectado el balanceado de la clase `isFraud` puesto que este valor indica que no existe una mayor representación de una clase y por ende el clasificador no se ve influido por una clase mayoritaria.

Si nos fijamos en un campo denominado *Kappa* podemos conocer cuán bueno es el clasificador entrenado frente a otro que escoge las etiquetas al azar. Investigando más acerca de la repercusión de este valor [12] se considera que un modelo es confiable cuando este valor se encuentra **por encima del 80%**. En este caso podemos apreciar que nuestro valor es del 55%, lo que significa que **falla en la mitad de las ocasiones**. Tal y como se explica en la fuente [12], la gravedad de estos resultados dependen del ámbito en el que se encuentre y considerando la capacidad de detectar transacciones fraudulentas, podemos determinar que **este clasificador no es para nada competitivo** dentro de un ámbito real.

```{r}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_nb$preds, datos$test$isFraud)
```

Visualizando la curva ROC podemos comprobar la teoría anterior puesto que tal y como podemos observar el clasificador no dispone de una gran capacidad de generalización puesto que su área apenas llega a un 77%. 

A continuación vamos a repetir el mismo proceso pero utilizando **Random Forest** y los mismos conjuntos de datos de entrenamiento y validación. 

```{r message=FALSE, warning=FALSE}
# MODELO RANDOM FOREST
set.seed(32)
library(randomForest)
# Entrenamos el modelo utilizando todas las características
modelo_rf<-randomForest(formula=isFraud~., data = datos$train, ntree=100)
# Obtenemos las predicciones y matriz de confusión
calidad_rf<-calidad_modelo(modelo_rf, datos$test, datos$test$isFraud)
confusionMatrix(calidad_rf$conf, positive='1')
```

Tal y como podemos observar este modelo cuenta con una **precisión del 84%**, 7 puntos más que el modelo anterior. Si observamos, además, la matriz de confusión podemos apreciar una disminución de los falsos positivos lo que acompaña a esta mejora en la tasa de aciertos. Sin embargo, analizando de nuevo el resto de atributos según [12] en este caso el campo *Kappa* muestra aproximadamente un **69% de acierto** con respecto al clasificador entrenado por el método en cuestión. Si bien sigue sin llegar al recomendable 80%, podemos afirmar que este modelo muestra una mayor confiabilidad de cara a utilizarlo en un ámbito real comparado con el anterior. Aún así, con **este preprocesamiento ninguno de los modelos entrenados son competitivos**. 

```{r}
# Dibujamos la curva ROC del modelo de Naive Bayes
curva_roc(calidad_rf$preds, datos$test$isFraud)
```

Si además visualizamos la curva ROC del modelo entrenado con Random Forest podemos apreciar una mayor amplitud, lo que produce un **área mayor bajo la curva**, por lo que bajo el mismo preprocesamiento de los conjuntos esta técnica ha conseguido un clasificador con una mejor capacidad de generalización frente a la técnica Naive Bayes utilizada para el primer modelo.

## Modelos del segundo preprocesamiento



# Bibliografía

[1] Kaggle, IEEE-CIS Fraud Detection, https://www.kaggle.com/c/ieee-fraud-detection/data

[2] RDocumentation, df_status, https://www.rdocumentation.org/packages/funModeling/versions/1.9.3/topics/df_status

[3] Kaggle, Data Description (Details and Discussion), https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203

[4] Andrie de Vries and Joris Meys, How to Take Samples from Data in R, https://www.dummies.com/programming/r/how-to-take-samples-from-data-in-r/

[5] Documentación sobre la función nearZeroVar, https://www.rdocumentation.org/packages/caret/versions/6.0-85/topics/nearZeroVar

[6] Documentación sobre la función mice, https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice

[7] Documentación acerca de la función findCorrelation, https://rdrr.io/rforge/caret/man/findCorrelation.html

[8] r-statistics.co, Selva Prabhakaran, Outlier Treatment, http://r-statistics.co/Outlier-Treatment-With-R.html

[9] Documentación sobre la función boxplot, https://www.rdocumentation.org/packages/grDevices/versions/3.6.2/topics/boxplot.stats

[10] Documentación de la función confusionMatrix, https://www.rdocumentation.org/packages/caret/versions/3.45/topics/confusionMatrix

[11] DEGREES OF BELIEF, COMMON EVALUATION MEASURES FOR CLASSIFICATION MODELS, https://degreesofbelief.roryquinn.com/common-evaluation-measures-for-classification-models

[12] DATANOVIA, INTER-RATER RELIABILITY MEASURES IN R: Cohen’s Kappa in R: For Two Categorical Variables
, https://www.datanovia.com/en/lessons/cohens-kappa-in-r-for-two-categorical-variables/#interpretation-magnitude-of-the-agreement
