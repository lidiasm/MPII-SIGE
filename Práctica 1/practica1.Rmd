---
title: "Práctica 1. Pre-procesamiento de datos y clasificación binaria."
author: "Lidia Sánchez Mérida"
output: pdf_document
---

# Conjuntos de datos

En esta práctica se procede a analizar el conjunto de datos procedente de *Kaggle* y denominado **IEE-CIS Fraud Detection** [1]. En él se distinguen hasta dos tipos de documentos. Aquellos que contienen el término `identity` contienen los datos personales de los individuos que han realizado transacciones bancarias. Mientras que los ficheros que contienen la palabra `transaction` disponen de la información asociada a cada una de las transacciones realizadas. Ambos ficheros son relacionables a través de un campo denominado `TransactionID`, el cual nos permite conocer más detalles acerca de las personas que han realizado las transferencias. Sin embargo, de muchas de ellas no se conoce esta información.

Para cada uno de estos tipos de ficheros existe su correspondiente conjunto de entrenamiento y validación ya separados, pero como todos los conjuntos de datos son tan amplios voy a usar directamente los ficheros `train_innerjoin.csv` y `test_innerjoin.csv` proporcionados en el repositorio de GitHub de la asignatura.

```{r message=FALSE, warning=FALSE}
# Establecemos una semilla para que los resultados sean reproducibles.
set.seed(0)
# Leemos los datos desde los ficheros
train<-read.csv(file="./train_innerjoin.csv", header=TRUE, sep=",")
test<-read.csv(file="./test_innerjoin.csv", header=TRUE, sep=",")
# Dimensiones de los conjuntos
dim(train)
dim(test)
```

Tras cargar los datos podemos observar, como anteriormente he puntualizado, que ambos cuentan con un número considerablemente amplio tanto de registros como de variables. Para conocer más información acerca de los conjuntos de datos vamos a realizar un análisis exploratorio que nos permita identificar los principales aspectos más relevantes.

# Análisis exploratorio

## Estado de los datos

Primeramente vamos a conocer el estado de los datos. Mediante la función `df_status` [2] podremos conocer los valores de todos los campos de un conjunto con el objetivo de conocer la cantidad y el porcentaje de ceros, valores nulos o infinitos. Asimismo, en la última columna también nos indica la cantidad de valores únicos que existen para cada campo. De este modo podemos conocer, por ejemplo, si una variable es categórica. Para conocer cuáles podrían ser los campos de mayor relevancia, podemos leer desde el propio Kaggle qué es lo que representa cada uno de ellos [3].

```{r message=FALSE, warning=FALSE, include=FALSE}
# Obtenemos el estado de los dos conjuntos de transacciones
library(funModeling)
train_st<-df_status(train)
test_st<-df_status(test)
```

Si bien el resultado proporcionado por estas funciones es súmamente extenso como para imprimirlo, he realizado un análisis acerca de las variables más relevantes obteniendo las siguientes conclusiones:

* En primer lugar destacamos que la variable categórica `isFraud` que intentamos predecir dispone de un altísimo número de ejemplos de transacciones no fraudulentas en el conjunto de entrenamiento. Tal es así que apenas existe un 8% de transacciones clasificadas como fraudulentas. Si bien este tipo de fenómenos es bastante común, clases tan súmamente desbalanceadas suelen dificultar el proceso de entrenamiento y obtención de buenos clasificadores.
* Casi la mitad de las variables disponen de aproximádamente un 50% de valores perdidos. Mayoritariamente se encuentran relacionadas con medidas tales como la distancia, características como la dirección, entre otras.
* Existen bastantes variables categóricas, además de `isFraud` que indican los dominios origen y destino, el tipo de dispositivo desde el que se hizo la transacción, el producto de la misma, entre otras.

## Relación dinero-fraude

A continuación procedo a representar gráficamente otro tipo de estadísticas que pueden también ser interesantes para conocer, un poco, las características del tipo de transacciones. En primer lugar vamos a averiguar si existe algún tipo de **relación entre la cantidad de la transacción y su clasificación como fraudulenta o no**. El objetivo es conocer si existe un patrón para detectar las transacciones fraudulentas, como por ejemplo si en muchas de ellas se han traspasado grandes cantidades de dinero. Para ello vamos a dibujar un histograma para representar la variable `TransactionAmt`, que es la que contiene las sumas de dinero, y las diferenciaremos en función de si son o no fraudulentas. Para este último paso deberemos de transformar los valores de la columna `isFraud` a categóricos  

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(magrittr)
library(dplyr)
# Transformamos 0->No, 1->Yes para diferenciar las transacciones
# fraudulentas de las que no lo son.
train_categ<-train %>% mutate(isFraud = as.factor(ifelse(isFraud == 1, 'Yes', 'No')))
# Representamos las cantidades y si son de transacciones fraudulentas o no.
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionAmt),
     fill = as.factor(isFraud)), binwidth = 100)
```

Como se puede observar no existen grandes diferencias entre las cantidades monetarias de las transacciones clasificadas como fraudulentas de las que no lo son. Sin embargo, este histograma nos permite comprobar que la **mayoría de transacciones se encuentran agrupadas** dentro de un rango aproximado [0-700], lo cual nos posibilita realizar un **corte horizontal para reducir el número de registros** a los contenidos en dicho rango.

## Relación tiempo-fraude

Aprovechando los dos conjuntos separados de transacciones, vamos a estudiar como segundo caso la **relación entre el tipo de transacción y la medida de tiempo asociada** en el campo `TransactionDT`. Esta representa la diferencia entre dos valores temporales, y aunque si bien en la descripción del dataset no especifican las medidas utilizadas, sus valores se basan en los de los campos DX donde X es un número entre 1 y 15. De estos conocemos por la descripción en Kaggle que se consideran, por ejemplos, los días transcurridos entre una transacción y otra. El objetivo de este análisis consiste en averiguar si el rango temporal entre dos transacciones puede ser un factor a considerar para averiguar si son o no fraudulentas.

```{r}
ggplot(train_categ) + 
  geom_histogram(aes(x = as.numeric(TransactionDT),
    fill = as.factor(isFraud)), binwidth = 1000000)
```

Tal y como se puede observar, aquellas transacciones marcadas como **fraudulentas disponen de un valor menor que las no fraudulentas**, por lo que podemos considerar que quizás este factor de tiempo puede estar relacionado con la naturaleza legal de la transacción. Una posible teoría que explique este suceso puede basarse en que cuando, por ejemplo hacemos algo que está mal, intentamos que sea lo más rápidamente posible. En análisis posteriores comprobaremos si efectívamente existe algún tipo de relación entre esta variable y la columna a predecir que nos ayude a entrenar modelos predictivos.

# Preprocesamiento de datos

En esta sección procedemos a aplicar diversas técnicas para mejorar la calidad del conjunto de datos de modo que podamos obtener un subconjunto más sencillo con el que entrenar los futuros modelos predictores. Uno de los principales problemas de este conjunto reside en su **gran dimensionalidad** tanto del número de registros como de columnas. El hecho de disponer de un enorme grupo de muestras puede provocar problemas de rendimiento a la hora de intentar entrenar un modelo con tal volumen de datos. Mientras que disponer de tal cantidad de variables también puede producir grandes tiempos de espera en entrenamiento al considerar tantas columnas para obtener el clasificador. Estos son los dos principales problemas que se abordan a continuación.

## Reducción del número de muestras aleatoriamente

En primer lugar debo reducir el número de registros del conjunto de datos para poder trabajar con él. Para ello, en primer lugar, vamos a realizar el corte horizontal hablado anteriormente y referente al **análisis exploratorio de la cantidades monetarias** de las transacciones. De este modo obtendremos un conjunto de datos más reducido y orientado hacia el grupo mayoritario de transacciones. Para ello vamos a eliminar aquellos registros con un cantidad superior a 700 USD.

```{r}
library(dplyr)
library(magrittr)
minitrain_700<-train %>% filter(TransactionAmt < 700)
cat("Dimensiones al quitar transacciones > 700 USD\n")
dim(minitrain_700)
# Escogemos 50.000 muestras aleatoriamente del conjunto de entrenamiento
minitrain<-minitrain_700[sample(nrow(minitrain_700), 50000), ]
```

Tal y como podemos comprobar, hemos conseguido reducir casi 1000 registros. Sin embargo, este conjunto de datos sigue siendo inmanejable por lo que es necesario aplicar una reducción aún mayor. Para ello vamos a realizar un muestreo aleatorio utilizando la función `sample` [4]. El objetivo es seleccionar 50.000 individuos de forma aleatoria incluyendo todas las columnas del dataset. De este modo podremos obtener un conjunto de datos más sencillo con el que comenzar a trabajar.

## Variables con demasiados valores perdidos

Comenzamos abordando otro de los principales problemas de este conjunto de datos que consiste en la aparición numerosa de valores perdidos o NAs. Si bien uno de los métodos posibles para eliminarlos es predecirlos utilizando alguna técnica que nos permita obtener un valor aproximado a partir de la información que existe en el conjunto, si el número de estos valores es sumamente predominante **(> 50%)** es prácticamente imposible aplicar este método. Por lo tanto, a continuación se desarrolla una función en la que se eliminarán aquellas columnas del dataset que dispongan de un número de valores perdidos superior al 50% de los datos totales. Para ello me he basado en los ejemplos de los *scripts* proporcionados en la asignatura.

```{r message=FALSE, warning=FALSE, include=FALSE}
eliminar_columnas_nas<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(funModeling)
  estado<-df_status(datos)
  # Obtenemos aquellas columnas con más de un 50% de valores perdidos (NAs) a partir del estado
  na_cols <- estado %>%
    filter(p_na > 50) %>%
    select(variable)
  # Eliminamos las columnas obtenidas
  remove_cols <- bind_rows(
    list(na_cols)
  )
  # Obtenemos el conjunto de datos reducido 
  datos_finales <- datos %>%
    select(-one_of(remove_cols$variable))
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Variables irrelevantes

Analizando más en profundidad la información que nos aportan las columnas desde la propia página del dataset [1] me he percatado de que las variables **C1-C14 están cifradas**, lo cual quiere decir que si no disponemos de la clave de encriptado no podemos acceder a la información real, y por tanto, disponemos de 14 columnas con información que no significa nada. Por ello, como segundo paso de este preprocesamiento se desarrolla la siguiente función que se encarga de eliminar dichas columnas dado un dataset.

```{r}
eliminar_columnas_encriptadas<-function(datos) {
  # Eliminamos las columas C1-C14
  datos_finales<-datos[, -which(colnames(datos) %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7", "C8", "C9", "C10", "C11", "C12", "C13", "C14"))]
  # Devolvemos el conjunto resultante
  datos_finales
}
```

También se consideran variables irrelevantes aquellas que no aportan información útil debido a que sus **valores son muy similares**. Este tipo de columnas no nos ayudan a predecir, posteriormente, si una transacción es o no fraudulenta puesto que independientemente de las muestras su comportamiento no varía. Uno de los métodos por los que se puede decidir si una variable dispone de valores interesantes es mediante la **desviación estándar**. Si esta medida estadística es 0 entonces nos indica que no existe variabilidad en los valores de la columna, y por tanto, esta variable no será útil. 

Para realizar este preprocesamiento vamos a hacer uso de la función `nearZeroVar` [5] que calcula el número de valores únicos y su frecuencia de aparición. De este modo obtendremos aquellas variables cuya desviación estándar sea 0 o cercana a dicho valor. Previo a su aplicación, es necesario transformar todas las variables a numéricas para poder calcular la desviación estándar. Para las columnas categóricas se asignará a cada etiqueta un único valor que la represente.

```{r}
eliminar_columnas_sd<-function(datos, eliminar_todas) {
  # Convertimos todas las variables a variables numéricas
  for(i in c(1:ncol(datos))) {
       datos[,i] <- as.numeric(datos[,i])
  }
  # Calculamos el número de variables con desviación estándar cercana o igual a 0.
  library(caret)
  nz<-nearZeroVar(datos, names = TRUE, saveMetrics = TRUE)
  # Desviación estándar = 0
  nz0<-nz[nz[,"zeroVar"] > 0, ]
  # Desviación estándar cercana a 0
  nz_cercano0<-nz[nz[,"zeroVar"] + nz[,"nzv"] > 0, ]
  # Eliminamos las variables con desviación estándar = 0
  datos_finales<-datos[, -which(colnames(datos) %in% rownames(nz0))]
  
  # Si eliminar_todas=TRUE entonces también eliminamos las variables con sd cercana a 0
  if (eliminar_todas == TRUE) {
    datos_finales<-datos_finales[, -which(colnames(datos_finales) %in% rownames(nz_cercano0))]
  }
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Tratamiento de valores perdidos

En este apartado procedemos a tratar los numerosos valores perdidos que contiene el conjunto de datos. Si bien existen multitud de técnicas para aplicarle a este tipo de datos, yo voy a considerar las siguientes:

* **Eliminación de NAs**. Esta puede ser la técnica más drástica para tratar los valores perdidos. Para llevarla a cabo vamos a aplicar la función `na.omit()` que eliminará todos los registros que contentan este tipo de valores.
* **Imputación de NAs**. Al contrario que la anterior, esta técnica más sofisticada es una de las más utilizadas según he estado investigando. Para aplicarla existen varias librerías pero al parecer la más popular es `mice` debido a las múltiples operaciones que puede llevar a cabo para asignar un valor a cada NA [6]. Dependiendo de la naturaleza de los datos, existen métodos específicos aunque también dispone de métodos generales aplicables a cualquier tipo de dato.

A continuación se expone el código de las funciones asociadas a las dos técnicas explicadas anteriormente.

```{r}
eliminar_nas<-function(datos) {
  datos_finales<-na.omit(datos)
  # Devolvemos el conjunto resultante
  datos_finales
}

imputar_nas<-function(datos, imputs, iters, metodo) {
  library(mice)
  # Especificamos el número de imputaciones, el número de iteraciones por imputación y el método a utilizar para imputar los NAs
  modelo_mice<-mice(minitraint_sd, m=imputs, maxit=iters, method=metodo)
  # Obtenemos el conjunto de datos imputado
  datos_imputados<-complete(modelo_mice)
  # Comprobamos el número de NAs que han quedado
  anyNA(datos_imputados)
  # Devolvemos el conjunto resultante
  datos_imputados
}
```

## Correlación

En esta sección vamos a estudiar la **correlación existente entre las variables y la columna a predecir**, que en el caso de este *dataset* es `isFraud`, y posteriormente analizaremos cuán **relacionadas se encuentran las variables entre sí**. El objetivo es, por un lado, conocer si existe alguna variable o grupo de variables que expliquen el comportamiento de la variable categórica a predecir. Mientras que por otro lado queremos conocer si existen variables muy correladas entre sí de manera que podamos reducir la dimensionalidad eliminando aquellas cuyo coeficiente de correlación sea muy alto, ya que este sería un indicio de que dichas columnas aportan la misma información.

Para el primer estudio vamos a aplicar la función `correlation_table` tal y como se ejemplifica en uno de los *scripts* proporcionados en la asignatura. Como precondición para calcular los coeficientes de correlación, es necesario que las columnas dispongan solo de valores numéricos. 
De igual forma, esta condición es necesaria para el segundo análisis que involucra a todas las variables, para el cual usaremos, en primer lugar, la función `cor` que se encargará de calcular los coeficieintes de correlación entre todas las variables del conjunto. Una vez dispongamos de esta matriz, aplicaremos la función `findCorrelation` [7] que nos ayudará a identificar aquellas variables que tienen un coeficiente de correlación especificado como argumento. De este modo se puede personalizar el nivel de exigencia con el que eliminar las columnas correladas.

A continuación se presentan las funciones asociadas a los dos estudios de correlación que se realizarán a posteriori.

```{r message=FALSE, warning=FALSE}
correlacion_fraude<-function(datos) {
  library(funModeling)
  # Calculamos los coeficientes de correlación en torno a la variable clasificatoria
  tabla_corr<-correlation_table(datos, target='isFraud')
  tabla_corr
}

correlacion_variables<-function(datos, max_corr) {
  library(caret)
  # Calculamos los coeficientes de correlación todas-todas las variables.
  coefs<-cor(datos)
  # Mostramos las columnas con un coeficiente de correlación superior al 90%.
  ## Excluimos las filas con valores perdidos porque si no esta función no se puede aplicar.
  cols_corr<-findCorrelation(na.omit(coefs), cutoff=max_corr)
  cols_corr
  # Eliminamos las columnas resultantes del dataset
  datos_finales<-datos[-c(cols_corr)]
  # Devolvemos el conjunto resultante
  datos_finales
}
```

## Outliers

Los *outliers* son datos cuyos valores están caracterizados por encontrarse fuera del rango normalizado para el atributo. En otras palabras, este tipo de datos disponen de valores muy diferentes a los del resto del conjunto y por ende pueden afectar a la capacidad de generalización de los modelos entrenados. Por este motivo, esta sección se va a dedicar al estudio y tratamiento de este tipo de valores. Para el análisis me he inspirado en este ejemplo [8] en el que se hace uso de la función `boxplot` [9], con la que se calculan los valores situados fuera del rango particular de cada variable. De este modo podremos conocer los *outliers* de cada una de las columnas.

```{r message=FALSE, warning=FALSE}
estudio_outliers<-function(datos) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Calculamos los outliers de todas las variables del conjunto de datos.
  outliers<-datos %>%
        map(~ boxplot.stats(.x)$out) 
  # Mostramos los 10 primeros resultados
  head(summary(outliers), 10) 
}
```

Una vez disponemos de información acerca de los *outliers* existentes en el conjunto de datos, procedemos a estudiar los diferentes tratamientos que se les pueden aplicar. Dentro del ejemplo mecionado anteriormente se presentan varias técnicas, de las cuales he escogido las dos más interesantes a mi parecer: **la imputación mediante una determinada fórmula y la predicción de los valores**. La primera técnica consiste en reemplazar aquellos valores que se sitúen fuera del rango de los cuantiles 25 y 75, por el cuantil 5 o el 95, respectivamente. El objetivo es transformar los *outliers* que se encuentran por debajo del rango en el valor más pequeño que se encuentra dentro de dicho intervalo, y realizar el mismo procedimiento con los que se encuentran por encima del rango pero, en este caso, asignando el mayor valor posible.

La segunda técnica consiste en convertir todos los *outliers* detectados en valores NA para posteriormente imputarlos. Como ya disponemos de una función que se encarga de tratar los valores perdidos, será esa que reutilizaremos para predecir este tipo de valores y así probar esta segunda técnica. A continuación se presentan las funciones asociadas a los dos tratamientos explicados anteriormente.

```{r message=FALSE, warning=FALSE, include=FALSE}
outliers_formula<-function(datos) {
  for(col in datos) {
    # Calculamos los cuantiles 25 y 75 para comprobar es un outlier
    quantiles<-quantile(var, probs=c(0.25, 0.75))
    # Calculamos los cuantiles 5 y 95 para asociar como nuevos valores a los outliers
    nuevos_valores<-quantile(col, probs=c(0.05, 0.95))
    # Calculamos la varianza máxima que puede sufrir un valor
    H<-1.5*IQR(col)
    # Sustitución de outliers
    col[col < (quantiles[1] - H)] <- nuevos_valores[1]
    col[col > (quantiles[2] + H)] <- nuevos_valores[2]
    # Actualizamos el dataset
    datos[col]<-col
  }
}

outliers_prediccion<-function(datos, imputs, iters, metodo) {
  library(magrittr)
  library(dplyr)
  library(tidyr)
  library(purrr)
  # Eliminamos la columna isFraud para que no se le aplique este procedimiento.
  datos_sin_fraud<-datos[, -which(names(datos) %in% c("isFraud"))]
  # Transformamos los valores outliers en valores perdidos (NAs)
  datos_nas<-as.data.frame(apply(datos_sin_fraud, 2, function(row){row[row %in% boxplot(row, plot = FALSE)$out] = NA; row})) 
  datos_finales<-imputar_nas(datos_nas, imputs, iters, metodo)
  # Devolvemos el dataset resultante
  datos_finales
}
```

## Selección de variables

El objetivo de este apartado consiste en reducir la dimensionalidad del *dataset* en lo referente a las variables que nos permiten predecir la columna categórica `isFraud`. Si bien existen diferentes técnicas para llevar este procedimiento a cabo, vamos a utilizar una de las más populares denominada **Análisis de Componentes Principales (PCA)**. Su procedimiento consiste en realizar diversas combinaciones lineales de los atributos del dataset de modo que nos indique cuáles son los más relevantes a considerar. Este tipo de técnicas se deben aplicar solo cuando el número de variables es considerablemente amplio, puesto que si lo llevamos a cabo con un número de columnas razonable podemos perder información al eliminar algunas de ellas. 

Si bien existen dos paquetes principales para aplicar el PCA con y sin valores perdidos, vamos a hacer uso de este último puesto que en esta sección se presupone que el conjunto de datos se encuentra preprocesado e imputado. Asimismo, el PCA se puede ver altamente influido por aquellas variables con una mayor varianza lo que supondría su predominio sobre el resto de columnas. Es por ello por lo que el planteamiento de esta técnica va a ser similar al de este ejemplo [12]. En él se utiliza la función `prcomp()` [13] que previo a la aplicación del algoritmo PCA, es capaz de analizar las variables y aplicar procedimientos de **centrado y escalado** para que su media sea 0 y su desviación estándar 1.

**HAY GRÁFICOS EN LOS EJEMPLOS BUSCADOS Y EN EL TRABAJO3 - Al aplicar finalmente el algoritmo PCA (que lo haremos directament con \textbf{preProcess()}), obtendremos un conjunto reducido de "atributos". Lo expresamos entre comillas, ya que en realidad, estos valores son combinaciones lineales de los atributos iniciales, por tanto, como resultado habremos conseguido reducir en dimensionalidad a costa de tener datos mucho menos interpretables.**
**APLICAREMOS LUEGO PREPROCESS Y PREDICT PARA OBTENER EL CONJUNTO DE DATOS REDUCIDO?????????**

```{r}
```

## Balanceo de la clase a predecir

En esta última sección se tratará el desbalanceo de la clase `isFraud` con la que debemos entrenar los modelos para poder predecir si una transacción es fraudulenta o no. Como hemos podido observar en el análisis exploratorio, la gran mayoría de muestras son transacciones no fraudulentas (cerca del 96%) mientras que la clase minoritaria es la asociada a las transacciones fraudulentas. Este tipo de problemas es bastante común, pero en el caso de este dataset **es súmamente desproporcionado**. Para intentar paliar este inconveniente se van a aplicar dos de las técnicas más populares: **downsampling y (?)**. En el primer caso se obtendrá un subconjunto de datos en el que se iguale el número de muestras de ambas clases, es decir, se escogerán tantas muestras de transacciones no fraudulentas al azar como ejemplos de transacciones fradulentas haya [downsample]. Con esta técnica el conjunto de datos se reducirá drásticamente, lo cual provocará una serie de consecuencias que analizaremos posteriormente al entrenar los modelos.

```{r}
downsampling<-function(datos) {
  library(caret)
  # Obtenemos las etiquetas a predecir
  etiquetas<-factor(datos$isFraud)
  # Downsampling para igualar ambas clases
  datos_finales<-downSample(datos, etiquetas, yname='isFraud')
  # Comprobamos que ambas clases se encuentran balanceadas
  count(datos_finales, isFraud)
  # Devolvemos los datos resultantes
  datos_finales
}
# set.seed(1)
# sample_iris <- iris %>%
#   group_by(Species) %>%
#   sample_n(10)

```

# Modelos

Esta sección se encuentra orientada a producir diversos modelos predictivos utilizando varias combinaciones de las técnicas de preprocesamiento explicadas anteriormente. El objetivo es estudiar si la capacidad de generalización de los modelos mejora o empeora en función de los diferentes cambios de preprocesamiento que introduzcamos. Para todas las combinaciones vamos a aplicar los mismos métodos de entrenamiento para que los modelos obtenidos sean comparables y así basar el análisis en función de las técnicas de preprocesamiento de datos. 
>>>>>>>>>> EXPLICAR TÉCNICAS DE ENTRENAMIENTO Y LAS LIBRERÍAS A UTILIZAR >>>>>>>>>>>>>>>>>>>>>>>>>>

## Modelo 1

En este primer modelo vamos a comenzar eliminando aquellas columnas con más de un **50% de valores perdidos**. De este modo conseguimos reducir el número de variables de **434 a 305**, que si bien siguen siendo muchas solo eliminando las columnas con más de un 50% de NAs hemos conseguido una buena reducción del número de columnas. A continuación, procedemos a eliminar las **columnas consideradas irrelevantes** para continuar reduciendo la dimensionalidad del conjunto de modo que eliminando las columnas encriptadas y las que tienen desviación estándar 0 se consigue disponer de **289 columnas**.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Eliminamos columnas con > 50% NA
minitrain_m1<-eliminar_columnas_nas(minitrain)
# Eliminamos columnas irrelevantes: C1-C14 encriptadas y con sd=0
minitrain_m1<-eliminar_columnas_encriptadas(minitrain_m1)
minitrain_m1<-eliminar_columnas_sd(minitrain_m1, FALSE)
```

Una vez hemos reducido casi la mitad de las columnas del *dataset*, previo a realizar estudios más sofisticados vamos a **tratar los valores perdidos**. Para ello vamos a aplicar la primera técnica explicada anteriormente consistente en **eliminar todos los registros con NA**. De este modo podemos comprobar cómo el conjunto de datos se ve grávemente afectado puesto que pasa de 50.000 muestras a disponer solo de aproximadamente **7.400 ejemplos**. Como ya anticipamos este método es muy agresivo pero cuando entrenemos los modelos comprobaremos si este hecho se ve reflejado en su calidad.

```{r}
# Eliminar registros con valores NA
minitrain_m1<-eliminar_nas(minitrain_m1)
# Dimensión del conjunto resultante
dim(minitrain_m1)
# Correlación entre variables~isFraud
head(correlacion_fraude(minitrain_m1), 10)
```


# CONCLUSIONES
------------------------------------------------------------------------------------------------------


### Correlación entre variables

En primer lugar vamos a analizar qué variables son las más **relacionadas con la columna clasificatoria `isFraud`**. Cuanto mayor sea el coeficiente de correlación entre una variable y la etiqueta, más nos ayudará dicha variable a predecir si la transacción es fraudulenta o no. 


Si bien solo se muestran las 10 primeras filas puesto que aún hay muchas variables, examinándola particularmente la única variable relacionada con la variable clasificatoria es ella misma, mientras que el resto de variables apenas se encuentran correladas con la misma. Esto significa que no existe un claro grupo de variables que expliquen si una transacción es fraudulenta o no, por lo que con estos resultados a priori no podemos aplicar ninguna reducción adicional.

A continuación vamos a realizar un estudio de **correlación entre todas las variables**. El objetivo es identificar si existen variables estrechamente relacionadas entre sí que puedan afectar, posteriormente, a los futuros clasificadores que vamos a entrenar. 

Tal y como podemos observar, existen 17 columnas con un **coeficiente de correlación superior al 90%**. Por lo general esto nos indica que todas ellas aportan la misma información, y por ende, si las dejamos en el conjunto de datos tendremos un montón de columnas proporcionando información redundante.

### Outliers
Como podemos comprobar, la mayoría de las variables cuenta con varios registros con valores *outliers* por lo que será relevante tratarlos de algún modo antes de entrenar los clasificadores. También es necesario destacar que algunas de ellas son variables categóricas cuyas etiquetas se han transformado en valores numéricos. Por lo que tanto variables numéricas como categóricas disponen de *outliers*. 
Investigando un poco acerca de los mejores métodos para tratar con este tipo de valores [10], pienso que uno de los más efectivos consiste en transformar estos *outliers* en valores perdidos o NAs. El objetivo es predecirlos utilizando alguna técnica que permita entrenar un modelo predictivo capaz de asignar un valor a cada uno de esos NAs que representan los *outliers*. Este procedimiento se aplicará a todas las variables **excepto a la variable `isFraud`** puesto que su tratamiento será realizado particularmente considerando que es la variable que tenemos que predecir.

Según he estado investigando en diversas fuentes, al transformar los *outliers* a valores NA quizás se hayan generado demasiados valores perdidos en ciertas columnas y por tanto le ha sido imposible a la función imputarlos, puesto que no cuenta con información suficiente como para poder asignarles un valor a cada uno de ellos. Para confirmar esta teoría he visualizado un resumen de los datos antes de transformar los *outliers* a NA con la función `summary`. Si bien no voy a poner los resultados puesto que ocupan bastante espacio, cabe destacar que todas las variables que han resultado tener NA tras este proceso disponen de una menor variabilidad. En ciertos casos este suceso llega al extremo de dejar a la variable con un único valor.
-------------------------------------------------------------------------------------------------------



```{r}
# library(plyr)
# # Calculamos el número de apariciones de cada etiqueta para cada variable categórica.
# frecuencias<-apply(mini_train_t_reduc4[c(5:11)], 2, count)
# 
# 
# 
# library(mice)
# prueba<-mini_train_t_reduc3
# miceMod <- mice(prueba, method="rf")  # perform mice imputation, based on random forests.
# miceOutput <- complete(miceMod)  # generate the completed data.
# anyNA(miceOutput)
# 
# 
# 
# 
# qnt <- quantile(mini_train_t_reduc4$ProductCD, probs=c(.25, .75), na.rm = T)
# H <- 1.5 * IQR(mini_train_t_reduc4$ProductCD, na.rm = T)
# mini_train_t_reduc4[mini_train_t_reduc4$ProductCD < (qnt[1] - H)]
# mini_train_t_reduc4$ProductCD[mini_train_t_reduc4$ProductCD > (qnt[2] + H)]
# 
# resultado<-mini_train_t_reduc4[mini_train_t_reduc4$ProductCD < (qnt[1] - H)]+mini_train_t_reduc4$ProductCD[mini_train_t_reduc4$ProductCD > (qnt[2] + H)]
```


# Bibliografía

[1] Kaggle, IEEE-CIS Fraud Detection, https://www.kaggle.com/c/ieee-fraud-detection/data
[2] RDocumentation, df_status, https://www.rdocumentation.org/packages/funModeling/versions/1.9.3/topics/df_status
[3] Kaggle, Data Description (Details and Discussion), https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203
[4] Andrie de Vries and Joris Meys, How to Take Samples from Data in R, https://www.dummies.com/programming/r/how-to-take-samples-from-data-in-r/
[5] Documentación sobre la función nearZeroVar, https://www.rdocumentation.org/packages/caret/versions/6.0-85/topics/nearZeroVar
[6] Documentación sobre la función mice, https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice
[7] Documentación acerca de la función findCorrelation, https://rdrr.io/rforge/caret/man/findCorrelation.html
[8] r-statistics.co, Selva Prabhakaran, Outlier Treatment, http://r-statistics.co/Outlier-Treatment-With-R.html
[9] Documentación sobre la función boxplot, https://www.rdocumentation.org/packages/grDevices/versions/3.6.2/topics/boxplot.stats




[5] Math Insight, Plotting line graphs in R, https://mathinsight.org/plotting_line_graphs_in_r#:~:text=The%20basic%20plot%20command&text=The%20plot%20command%20accepts%20many,(with%20the%20main%20argument).
[6] FactoMineR users, MissMDA error message, https://groups.google.com/forum/#!topic/factominer-users/VlAXWYwSpDw





[12] RPubs, Joaquín Amat Rodrigo, Análisis de Componentes Principales (Principal Component Analysis, PCA) y t-SNE, https://rpubs.com/Joaquin_AR/287787
[13] Documentación acerca de la función prcomp, https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prcomp
[] Documentación acerca de la función downSample, https://www.rdocumentation.org/packages/caret/versions/6.0-85/topics/downSample




[8] R-bloggers, thiagogm, Near-zero variance predictors. Should we remove them?, https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/

[10] r-statistics.co, Selva Prabhakaran, Outlier Treatment, http://r-statistics.co/Outlier-Treatment-With-R.html


-------------------------------------------------------------------------------------------------------------
# BIBLIO ÚTIL
* Para calcular las frecuencias de las clases y tratar los outliers de las variables categóricas: StackExchange, Outlier detection on categorical network log data
, https://datascience.stackexchange.com/questions/20586/outlier-detection-on-categorical-network-log-data

* Para aplicar el PCA con valores NA: Documenación sobre la librería missMDA, https://cran.r-project.org/web/packages/missMDA/missMDA.pdf
