---
title: "Deep Learning con conjunto de datos Titanic"
author: "Lidia Sánchez Mérida"
output:
  html_document:
    df_print: paged
html_notebook: default
---

# Introducción al problema 

Este cuaderno trata de resolver el tercer ejercicio propuesto que consiste en mejorar los resultados obtenidos con respecto a una primera versión, en la que se entrena un modelo con redes neuronales para predecir si los pasajeros del Titanic sobrevivieron o no. Para ello vamos a hacer uso del mismo fichero que contiene el conjunto de entrenamiento situado en el repositorio de GitHub de la asignatura.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(32)
```

# Estudio de los datos

Comenzamos leyendo el fichero de entrenamiento al completo y a continuación estudiamos el estado de los datos para comprobar aspectos tales como el porcentaje de valores perdidos en cada columna, además de cómo de balanceada está la clase a predecir. Como podemos observar existen **tres columnas con valores perdidos** con, aproximadamente un 0.2%, 20% y 77% respectivamente. En los dos primeros casos esta cifra no es demasiado elevada y por tanto se pueden aplicar técnicas para tratar de imputarlos, mientras que para la tercera columna el número de NAs es bastante superior al de valores normales por lo que en principio se descarta. 
Asimismo, también podemos observar algunas **columnas que no parecen ser relevantes** para determinar si un pasajero sobrevivió o no, como el identificador del registro, su nombre o el número de ticket. Estos atributos también pueden ser omitidos.

En relación al balanceado de la clase a predecir podemos observar que existe un **61.62% de casos negativos y el resto positivos**. Si bien podemos determinar que la clase se encuentra desbalanceada, no es tan exagerado como el *dataset* de la práctica 1.

```{r message=FALSE, warning=FALSE}
library(readr)
library(funModeling)
# Leemos el fichero de entrenamiento
train<-read_csv('train.csv')
# Estudiamos el estado de los datos
df_status(train)
```

# Preprocesamiento de datos

En base al análisis anteriormente realizado y al *script* de ejemplo para este ejercicio se proponen las siguientes técnicas de preprocesamiento.

* Se seleccionan los mismos **atributos predictivos** que en la primera versión de este ejercicio: `Pclass`, `Sex`, `Age`, `Fare`, puesto que conocemos su buena capacidad para predecir la variable `Survived`.

* **Imputamos los valores perdidos** utilizando la función `mice` [1] puesto que ofrece la posibilidad de aplicar distintos métodos para predecir dichos valores. Esto es muy útil puesto que a veces las técnicas más sofitiscadas como clasificación o árboles de regresión pueden o disponer de información suficiente como para predecir el valor y, por tanto, hay que recurrir a técnicas más sencillas como la media.

* **Balanceamos la clase `Survived`** aplicando la técnica conocida como *oversampling* de modo que generemos un mayor número de datos de la clase minoritaria. Para ello voy a utilizar la función `SMOTE` [2] puesto que es capaz de buscar un equilibrio entre el número de muestras de ambas clases. Además podremos disponer de un conjunto de datos más amplio, lo que puede suponer un plus a la hora de entrenar los modelos posteriormente.

* Dividimos el conjunto completo en **entrenamiento y validación** para poder entrenar las distintas redes neuronales y testearlas posteriormente para comprobar la calidad de los modelos.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}

conjunto_inicial<-function() {
  library(magrittr)
  library(dplyr)
  # Seleccionamos las columnas predictoras y cambiamos el tipo
  # de la columna 'Sex' a numérico
  datos_finales<-train %>%
    select(Survived, Pclass, Sex, Age, Fare) %>%
    mutate(Sex = as.numeric(as.factor(Sex)) - 1)
  datos_finales
}

imputar_nas<-function(datos, imputs, iters, metodo) {
  library(mice)
  # Especificamos el número de imputaciones, el número de iteraciones
  # por imputación y el método a utilizar para imputar los NAs
  modelo_mice<-mice(datos, m=imputs, maxit=iters, method=metodo)
  # Obtenemos el conjunto de datos imputado
  datos_imputados<-complete(modelo_mice)
  # Devolvemos el conjunto resultante
  datos_imputados
}

over_sampling<-function(datos, p_over, p_under, k_vecinos) {
  library(DMwR)
  # Oversampling con el algoritmo SMOTE para generar más muestras
  datos$Survived<-as.factor(datos$Survived)
  datos_finales<-SMOTE(Survived ~., datos, perc.over=p_over, perc.under=p_under, k=k_vecinos)
  # Devolvemos los datos resultantes
  datos_finales
}

get_train_test<-function(datos, porcentaje_train) {
  # Mezclamos los datos aleatoriamente
  set.seed(32)
  datos<-datos[sample(1:nrow(datos)), ]
  # Dividimos el conjunto en train y test
  library(caret)
  train<-createDataPartition(datos$Survived, p=porcentaje_train, list=FALSE, times=1)
  datos_train<-datos[train, ]
  datos_test<-datos[-train, ]
  # Devolvemos ambos dataframes en una lista
  df<-list()
  df$train<-datos_train
  df$test<-datos_test
  df
}
```

# Modelos

En esta sección vamos a entrenar diversos modelos aplicando diferentes técnicas de preprocesamiento así como modificando las características de la red neuronal. En primer lugar, con el objetivo de comprobar la influencia del tratamiento de datos en la capacidad de generalización de los clasificadores, vamos a aplicar técnicas como la **imputación de NAs y el balanceado de clases para entrenar el modelo de ejemplo de GitHub**. A continuación, en función de los mejores resultados, estableceremos un mismo preprocesamiento para comenzar a **modificar la arquitectura y parámetros** de la red neuronal.

Para ello presentamos, a continuación, tres funciones con las cuales podremos obtener en primer lugar los conjuntos de entrenamiento y validación en formato matriz para entrenar la red neuronal. Este cometido lo lleva a cabo la segunda función a la que se le pasa el modelo de red generado así como el conjunto de entrenamiento. La última función tiene como fin validar el modelo préviamente entrenado proporcionando el conjunto de validación. Todos estos pasos son comunes al desarrollo de todos los modelos y por tanto se han modelado como funciones para no repetir código.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
get_conjunto_matriz<-function(datos) {
  library(magrittr)
  library(dplyr)
  # Matriz de datos sin las etiquetas
  x_datos <- datos %>%
    select(-Survived) %>%
    data.matrix()
  # Etiquetas
  y_datos <- datos %>%
    select(Survived) %>%
    data.matrix()
  # Devolvemos ambos
  resultado<-list()
  resultado$dataset<-x_datos
  resultado$etiquetas<-y_datos
  resultado
}

entrenar_modelo<-function(modelo, x_train, y_train, loss, op, eps, batch, val) {
  # Entrenamos el modelo
  set.seed(32)
  modelo %>% compile(
    loss = loss,
    metrics = c('accuracy'),
    optimizer = op
  )
  
  # Ajustamos el modelo a los datos de entrenamiento
  history <- modelo %>% 
    fit(
      x_train, y_train, 
      epochs = eps, 
      batch_size = batch,
      validation_split = val
    )
  plot(history)
}

validar_modelo<-function(modelo, x_test, y_test) {
  # Validación del modelo
  modelo %>% evaluate(x_test, y_test)
  preds<-modelo %>% predict_classes(x_test)
  conf_matriz<-confusionMatrix(as.factor(y_test), as.factor(preds), positive='1')
  conf_matriz
}
```

## Modelo 1

En este primer modelo procedo a **imputar los NAs** del conjunto de datos. Si bien es aconsejable que el número de imputaciones e iteraciones sea lo más alto posible para asegurar que el algoritmo converge, creo que la siguiente configuración es la que mejor relación guarda entre sendos atributos y el tiempo que invierte en la imputación. La calidad la podremos comprobar cuando entrenemos un modelo con estos datos. En cuanto a la técnica se ha escogido *Random Forest* por ser popular por su robustez y versatilidad con casi cualquier conjunto de datos.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
# Obtenemos el conjunto inicial con las mejores variables predictoras
train_orig<-conjunto_inicial()
# Imputamos los valores perdidos
train_imput<-imputar_nas(train_orig, 10, 10, 'rf')
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_imput, 0.7)
```

Como se puede observar, el conjunto de entrenamiento cuenta con 624 registros mientras que el de validación dispone de 267 muestras. A continuación transformamos sendos a matrices y separamos las etiquetas para, posteriormente, proporcionarle a la red neuronal los conjuntos resultantes y así ajustar el modelo. Como se ha mencionado anteriormente, el modelo es el mismo que el del ejemplo para comprobar la influencia de la imputación de los valores perdidos. Y tal y como podemos comprobar, el **modelo resultante es bastante similar al del *script* inicial con un 68.54% de precisión**. La razón de ser reside en que en este conjunto no existía un gran número de NAs comparado con la cantidad de valores de cada muestra, por lo que para el modelo entrenado con redes neuronales no ha supuesto un cambio significativo como para proporcionar resultados súmamente distintos. 

```{r message=FALSE, warning=FALSE}
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Reproducimos el mismo modelo que el del ejemplo
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo <- keras_model_sequential()
modelo <- modelo %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(ncol(train_imput) - 1)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo, m_train$dataset, m_train$etiquetas, 
    'binary_crossentropy', 'adam', 20, 100, 0.10)
validar_modelo(modelo, m_test$dataset, m_test$etiquetas)
```

## Modelo 2

En este segundo modelo vamos a incluir un nuevo preprocesamiento consistente en aplicar **oversampling** para balancear las clases de la variable a predecir. Asimismo, también se persigue aumentar el número de muestras para comprobar si con un mayor número de ejemplos, la capacidad de predicción del clasificador mejora. Con este fin vamos a partir del conjunto imputado anterior y aplicaremos directamente la función `SMOTE` para llevar a cabo esta técnica. Tras realizar varios experimentos para ajustar los parámetros de la función, a continuación se presenta la configuración con la que se logra disponer del mismo número de muestras positivas y negativas (2.052). En total el nuevo conjunto dispone de **4.104 registros**.

```{r message=FALSE, warning=FALSE, results='hide', echo=TRUE}
# Oversampling para balancear las clases
train_smote<-over_sampling(train_imput, 500, 120, 5)
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_smote, 0.7)
```

Lógicamente, al disponer de un mayor número de muestras al partir el conjunto original en entrenamiento y test estos disponen de un mayor número de ejemplos con los que entrenar y validar el clasificador, cuya configuración de nuevo será la misma que la del *script* inicial.

Como podemos observar, el clasificador entrenado en este caso proporciona **resultados son mucho peores** que en el caso anterior, ya que como podemos apreciar sus predicciones se basan en que todos sobrevivien y por tanto acierta un 50% de las veces, por estar las clases balanceadas. Por lo tanto, para este problema en concreto y esta tipología de red no es beneficioso aplicar *oversampling* sobre el conjunto de datos.

```{r message=FALSE, warning=FALSE}
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Reproducimos el mismo modelo que el del ejemplo
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo2 <- keras_model_sequential()
modelo2 <- modelo2 %>% 
  layer_dense(units = 32, activation = "relu", input_shape = c(ncol(train_smote) - 1)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo2, m_train$dataset, m_train$etiquetas, 
    'binary_crossentropy', 'adam', 20, 100, 0.10)
modelo2 %>% evaluate(m_test$dataset, m_test$etiquetas)
preds<-modelo2 %>% predict_classes(m_test$dataset)
length(preds[preds==0])
length(preds[preds==1])
```

## Modelo 3

En esta sección comenzamos a modificar la arquitectura de la red neuronal partiendo de un conjunto de datos al que se le han imputado los valores perdidos, pero mantiene el mismo número de registros. Este se corresponde con el conjunto de datos obtenido en el primer modelo entrenado.

El primer aspecto a considerar reside en establecer el número de capas de la red neuronal. Generalmente, la arquitectura más común es disponer de una capa de entrada, una capa oculta que realiza la extracción de características y una última capa, la de salida, que proporciona las predicciones para cada muestra. No obstante, dependiendo del problema que deseemos resolver esta arquitectura puede no ser la más adecuada. En nuestro caso, el número de variables predictoras es bastante bajo por lo que quizás este tipo de arquitectura más simple pueda obtener un mejor clasificador. Por ende vamos a establecer una capa de entrada cuyo número de neuronas es el mismo que el número de entradas, **una única capa oculta** de tipo *relu* y una última capa de salida. 

El número de neuronas de la capa oculta es también uno de los aspectos más relevantes a determinar puesto que afecta tanto en la calidad del modelo como en el tiempo invertido para entrenarlo. Para el desarrollo de este modelo he realizado un estudio en el que he ido variando el número de neuronas para comprobar el comportamiento de esta arquitectura. Para entrenar esta red vamos a establecer un **mayor número de épocas** para que la red disponga de más tiempo para la convergencia. Por otro lado, **disminuimos el tamaño de los minilotes** para que se realicen un mayor número de actualizaciones de los pesos y conseguir una convergencia más constante. Finalmente también he aumentado la porción del conjunto de datos que utiliza para la validación del modelo tras cada época. A continuación observamos la tabla resultante de los experimentos realizados para determinar el número de neuronas de la capa oculta.

| Calidad | 256 | 128 | 64 | 32 | 16 |
|---|---|---|---|---|
| Accuracy | 79.78% | 80.52% | 79.78% | 80.52% | 79.03% |
| Kappa | 58.14% | 59.92% | 58.30% | 60.22% | 57.16% |

Como podemos observar, **el número de neuronas óptimo para la capa oculta es de 32** puesto que es el que aporta una mejor precisión y coeficiente de *Kappa*. Este resultado es prácticamente alcanzado por el modelo con 128 neuronas pero el coste computacional de entrenarlo es muy superior al de 32 neuronas. Con este estudio podemos determinar que a mayor número de neuronas, no siempre se obtienen mejores resultados. La razón de ser reside en que si bien un mayor número de neuronas pueden descubrir un mayor número de características, estas no siempre son útiles para resolver el problema y por ende disminuye la capacidad de predicción del clasificador. Por lo tanto, para este tercer modelo establecemos 32 neuronas en la capa oculta. A continuación se muestran la gráfica del proceso de entrenamiento y las medidas de calidad correspondientes.

```{r message=FALSE, warning=FALSE}
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_imput, 0.7)
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Nuevo modelo: 3 capas, 1 oculta con relu
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo3 <- keras_model_sequential()
modelo3 <- modelo3 %>% 
   layer_dense(units = 32, activation = "relu", input_shape = c(ncol(datos$train) - 1)) %>%
   layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo3, m_train$dataset, m_train$etiquetas, 'binary_crossentropy', 'adam', 100, 5, 0.3)
validar_modelo(modelo3, m_test$dataset, m_test$etiquetas)
```

Si comparamos los resultados con los del primer modelo, podemos apreciar en la matriz de confusión que el número de aciertos aumenta en ambas clases y el número de falsos negativos se reduce considerablemente. Por lo tanto, con esta configuración se consigue que el modelo obtenido sea capaz de diferenciar mejor las características de un pasajero que sí sobrevivió al accidente. Acompañando este hecho, disponemos de un coeficiente de *Kappa* más elevado que nos indica que este modelo está utilizando más lo que ha aprendido durante el entrenamiento que etiquetando las muestras al azar, con respecto al primer modelo, el cual solo disponía de un coeficiente de 32.69%.

Adicionalmente también he realizado varias pruebas **variando el método de optimización** [3] con el objetivo de mejorar el resultado actual. Sin embargo, todas las técnicas probadas han resultado un poco peores excepto *Nadam* (*Nesterov Adam optimizer*) cuyo clasificador disponía de una precisión y coeficiente de *Kappa* similar a *Adam*. 

Siguiendo con las modificaciones de este modelo, he estado estudiando los parámetros de esta técnica de optimización para seguir mejorando el modelo. En primer lugar he variado la **tasa de aprendizaje** dentro del intervalo [0.001, 0.1], y he podido observar que a partir de 0.05 la precisión disminuye bastante. La razón de ser reside en que a mayor tasa de aprendizaje, menos profunda es la exploración del espacio y por tanto al acelerar el aprendizaje, podemos pasar por alto buenas soluciones.
Otro de los parámetros que he probado es `AMSGrad` [4], el cual ofrece una variante del algoritmo `Adam` que parte de una tasa de aprendizaje menor y dispone de cierta memoria para normalizar la evolución del gradiente. Para nuestro problema los resultados son bastante similares por lo que no merece la pena incluir este experimento.

De igual modo, también he experimentado con las distintas funciones de activación disponibles [5] para tratar de mejorar los resultados de este tercer modelo. Los resultados más destacables se exponen a continuación:

* `hard_sigmoid`. Es una variación de la función sigmoidal que calcula una aproximación lineal del exponente, por lo que es más rápida computacionalmente [6]. Sin embargo, solo variando este parámetro en la capa de salida hemos conseguido resultados mucho peores con **precisión 73.78% y Kappa de 47.73%**, por lo que podemos determinar que para nuestro problema no podemos utilizar esta aproximación para mejorar la capacidad de predicción del clasificador.

* `linear`. Aún sabiendo que no es de las mejores funciones de activación he querido probarla por curiosidad, y con este modelo he obtenido una **precisión del 77% y coeficiente de Kappa de 54%**. Esto nos indica que, pese a disponer de una función de activación más sencilla el clasificador obtenido prorporciona mejores resultados que en el caso anterior en el que se disponía de una función `relu` y una aproximación de la función sigmoidal. No obstante, no es un buen modelo puesto que el coeficiente de *Kappa* demuestra que aproximadamente en la mitad de ocasiones etiqueta las muestras al azar.

* `softplus`. Es una función cuyo comportamiento es muy similar a `relu` solo que, a diferencia de esta, `softplus` dispone de una función exponencial y otra logaritmica, lo que provoca una convergencia más constante y suave cuando es cercana a 0 que la `relu`, aunque esta última es mucho más eficiente de calcular [6] [7]. Sin embargo, con esta función hemos conseguido mejorar un poquito la calidad de este tercer clasificador por lo que su experimento se adjunta en un cuarto modelo.

Por último he intentado **añadir más capas ocultas** a la topología anterior, variando el número de neuronas en cada uno, y los resultados han sido peores que la arquitectura con una sola capa oculta. Este hecho también nos demuestra que no siempre añadir un mayor número de capas mejora el modelo predictivo. La razón de ser reside en un fenómeno denominado *overfitting* o sobreajuste por el cual la red se adapta demasiado a los datos de entrenamiento que pierde capacidad de generalización para datos que no ha visto aún. Este hecho sucede cuando existen un número de capas mayor que el necesario y el conjunto de datos no es suficientemente amplio. Como nuestro problema es sencillo y no dispone de muchas muestras, basta con una única capa oculta capaz de extaer la mayoría de características para entrenar un clasificador razonablemente bueno. 

## Modelo 4

En este último modelo se pretende mostrar la evolución del clasificador en su entrenamiento y validación así como las medidas de calidad del mismo. Para ello, como se ha comentado anteriormente, se ha utilizado la función de activación **`softplus` para la capa oculta** que es con la que he conseguido una ligera mejora con respecto al modelo anterior. 

Si establecemos la misma configuración, salvando la nueva función de activación, obtenemos un **79.03% de precisión y 57.08% de Kappa**. Como podemos observar estos resultados no son mejores que los anteriores. Sin embargo, tras varios experimentos, aumentando el doble la cantidad de neuronas de la capa oculta podemos alcanzar aproximadamente una **precisión de 81% y un coeficiente de *Kappa* de 60.4%**. La necesidad de aumentar el número de neuronas con esta función reside en la fórmula que aplica puesto que es más costosa computacionalmente que la función `relu`. Por lo tanto es necesario un mayor número de neuronas para extraer las características suficientes como para aprender a diferenciar los casos positivos de los negativos. Sin embargo, como se puede visualizar en la evolución del entrenamiento y validación, esta función experimenta una considerable mejora al poco tiempo de comenzar a entrenar el modelo. Mientras que la función `relu` parte de un tasa de precisión inicial más alta pero durante el proceso, para nuestro problema, apenas mejora.

De nuevo he intentado combinar esta configuración variando tanto los optimizadores como añadiendo más capas tanto con la misma función de activación como con `relu` pero los resultados siguen siendo algo peores, especialmente en el coeficiente de *Kappa* que puede bajar hasta un 5%.

```{r message=FALSE, warning=FALSE}
# Dividimos el conjunto en entrenamiento (70%) y validación (30%)
datos<-get_train_test(train_imput, 0.7)
cat("\nConjunto de entrenamiento\n")
dim(datos$train)
cat("\nDimensiones conjunto de validación\n")
dim(datos$test)

# Cambiamos el tipo de los conjuntos a matrices
m_train<-get_conjunto_matriz(datos$train)
m_test<-get_conjunto_matriz(datos$test)

# Nuevo modelo: 3 capas, 1 oculta con softplus
library(keras)
library(tensorflow)
# Semilla para que los modelos sean reproducibless
tensorflow::tf$random$set_seed(0)
modelo3 <- keras_model_sequential()
modelo3 <- modelo3 %>% 
  layer_dense(units = 64, activation = "softplus", input_shape = c(ncol(datos$train) - 1)) %>%
  layer_dense(units = 1, activation = "sigmoid")
# Entrenamos y validamos el modelo
entrenar_modelo(modelo3, m_train$dataset, m_train$etiquetas, 'binary_crossentropy', 'adam', 100, 5, 0.3)
validar_modelo(modelo3, m_test$dataset, m_test$etiquetas)
```

# Conclusiones

Como hemos podido observar a lo largo del cuaderno, en base a los diferentes experimentos tanto mostrados como no, podemos determinar que la conclusión principal reside en la sencillez del problema. Esta característica da lugar a que los modelos con topología y configuración más sencilla sean los que mejores resultados proporcionan en términos principales de precisión y coeficiente de *Kappa*, como ha sido el caso de los dos últimos. Estos modelo se caracterizan por disponer de una sola capa oculta, la cual es capaz de extaer las características necesarias para determinar si una persona ha sobrevivido o no al accidente del Titanic. Asimismo, no están compuestos por un número alto de neuronas por lo que no albergan demasiada complejidad, y por ende, no son muy costosos computacionalmente. Es por ello por lo que he podido realizar tantas pruebas.

Por otro lado hemos podido comprobar que por la baja cantidad de valores perdidos, el modelo entrenado con el *dataset* imputado no ha conseguido mejores resultados que el modelo que se muestra en el *script* inicial en el que se eliminan los NAs. Sin embargo, en la mayoría de los casos es mejor intentar predecir estos valores que eliminar los registros que los contengan porque el conjunto de datos podría quedar muy reducido. De igual modo hemos podido comprobar que aplicar *oversampling* con la función `SMOTE` para balancear la clase a predecir y generar un mayor número de muestras no ha sido beneficioso para entrenar los modelos, puesto que solo acertaba en la mitad de ocasiones. En resumen, los resultados proporcionados por cada uno de los cuatro modelos principales se puede observar en la siguiente tabla.

| Modelo | Precisión | Kappa |
|---|---|---|
| Inicial con imputación | 68.54% | 32.69% | 
| Inicial imputación+oversampling | 50% | - |
| 1 capa oculta `relu` | 80.52% | 60.22% |
| 1 capa oculta `softplus` | 80.9% | 60.42% |

# Bibliografía

[1] Documentación sobre la función mice, https://www.rdocumentation.org/packages/mice/versions/2.25/topics/mice

[2] Documentación sobre la función SMOTE, https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/SMOTE

[3] Keras Documentation, Usage of optimizers, https://keras.io/optimizers/

[4] Sashank J. Reddi, Satyen Kale & Sanjiv Kumar, ON THE CONVERGENCE OF ADAM AND BEYOND, http://www.satyenkale.com/papers/amsgrad.pdf

[5] Keras Documentation, Usage of activations, https://keras.io/activations/

[6] towards data science, Rinat Maksutov, 2018, Deep study of a not very deep neural network. Part 2: Activation functions, https://towardsdatascience.com/deep-study-of-a-not-very-deep-neural-network-part-2-activation-functions-fd9bd8d406fc

[7] Machine Intelligence, vivek, Different types of Activation functions in Deep Learning., 2017,  http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/